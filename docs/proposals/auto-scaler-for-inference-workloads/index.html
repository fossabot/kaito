<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-proposals/auto-scaler-for-inference-workloads" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.1.0">
<title data-rh="true">Auto Scaler For Inference Workloads In Kaito | Kaito</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://kaito-project.github.io/kaito/docs/img/kaito-logo.png"><meta data-rh="true" name="twitter:image" content="https://kaito-project.github.io/kaito/docs/img/kaito-logo.png"><meta data-rh="true" property="og:url" content="https://kaito-project.github.io/kaito/docs/proposals/auto-scaler-for-inference-workloads"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Auto Scaler For Inference Workloads In Kaito | Kaito"><meta data-rh="true" name="description" content="Auto Scaler for inference workloads in Kaito"><meta data-rh="true" property="og:description" content="Auto Scaler for inference workloads in Kaito"><link data-rh="true" rel="icon" href="/kaito/docs/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://kaito-project.github.io/kaito/docs/proposals/auto-scaler-for-inference-workloads"><link data-rh="true" rel="alternate" href="https://kaito-project.github.io/kaito/docs/proposals/auto-scaler-for-inference-workloads" hreflang="en"><link data-rh="true" rel="alternate" href="https://kaito-project.github.io/kaito/docs/proposals/auto-scaler-for-inference-workloads" hreflang="x-default"><link rel="stylesheet" href="/kaito/docs/assets/css/styles.e95ed9fd.css">
<script src="/kaito/docs/assets/js/runtime~main.b04ac157.js" defer="defer"></script>
<script src="/kaito/docs/assets/js/main.e30255a8.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return localStorage.getItem("theme")}catch(t){}}();null!==e?t(e):window.matchMedia("(prefers-color-scheme: dark)").matches?t("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,t("light"))}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}(),document.documentElement.setAttribute("data-announcement-bar-initially-dismissed",function(){try{return"true"===localStorage.getItem("docusaurus.announcement.dismiss")}catch(t){}return!1}())</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><div class="announcementBar_mb4j" role="banner"><div class="announcementBarPlaceholder_vyr4"></div><div class="content_knG7 announcementBarContent_xLdY">⭐️ If you like Kaito, please give it a star on <a target="_blank" rel="noopener noreferrer" href="https://github.com/kaito-project/kaito">GitHub</a>!</div><button type="button" aria-label="Close" class="clean-btn close closeButton_CVFx announcementBarClose_gvF7"><svg viewBox="0 0 15 15" width="14" height="14"><g stroke="currentColor" stroke-width="3.1"><path d="M.75.75l13.5 13.5M14.25.75L.75 14.25"></path></g></svg></button></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/kaito/docs/"><div class="navbar__logo"><img src="/kaito/docs/img/kaito-logo.png" alt="Kaito Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/kaito/docs/img/kaito-logo.png" alt="Kaito Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Kaito</b></a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/kaito-project/kaito" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link" aria-label="GitHub repository"></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><main class="docMainContainer_TBSr docMainContainerEnhanced_lQrH"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Title</h1>
<p>Auto Scaler for inference workloads in Kaito</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="summary">Summary<a href="#summary" class="hash-link" aria-label="Direct link to Summary" title="Direct link to Summary">​</a></h2>
<p>As the number of waiting inference requests increase, It is necessary to scale more inference instances in order to preventing to block inference requests. on the other hand, If the number of waiting inference requests declines, we should consider to reduce inference instances for improving gpu resource utilization.
Native Kubernetes has provided HPA capability to scale workload instance automatically as the metrics change. but HPA depends the third-party components(like prometheus, prometheus-adapter, etc.) to collect custom metrics from the source pods.</p>
<p>In this proposal, we hope to support a customized auto-sacler which is specialized for scaling GPU worklods for kaito. The auto-scaler is designed for a minimalistic configuration experience, with most parameters pre-tuned for optimal performance. This allows users to easily get started without requiring specialized knowledge of LLM.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="motivation">Motivation<a href="#motivation" class="hash-link" aria-label="Direct link to Motivation" title="Direct link to Motivation">​</a></h2>
<p>LLM inference service is a baisc and widly-used feature in Kaito, and Kaito community interest in auto scaler for inference workloads continues to intensify, related issues: <a href="https://github.com/kaito-project/kaito/issues/306" target="_blank" rel="noopener noreferrer">#306</a>, <a href="https://github.com/kaito-project/kaito/issues/1104" target="_blank" rel="noopener noreferrer">#1104</a>.</p>
<p>From the technical perspective, It&#x27;s a good idea to provide auto-scaler capability, becasue the auto-scaler of inference workloads dynamically adjusts the number of inference instances based on request volume--scaling up during traffic spikes to improve inference speed, and scaling down during low demand to minimize GPU resource waste.</p>
<p>To ensure ease of use, The specialized auto-scaler is hosted in a independent repo(kaito-project/llm-auto-scaler). at the same time, the llm-auto-scaler component can work with kaito without depending on any third-party components.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="goals">Goals<a href="#goals" class="hash-link" aria-label="Direct link to Goals" title="Direct link to Goals">​</a></h3>
<ul>
<li>llm-auto-scaler is a specialized auto-scaler for scaling gpu workloads automatically, and can integrate with kaito to work.</li>
<li>It is flexible to support mulitple scale strategies, and only one basic scale strategy(scaling workloads according to metrics change) is supported in the first version.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="non-goalsfuture-work">Non-Goals/Future Work<a href="#non-goalsfuture-work" class="hash-link" aria-label="Direct link to Non-Goals/Future Work" title="Direct link to Non-Goals/Future Work">​</a></h3>
<ul>
<li>Support cron scale strategy(like cron job) for llm-auto-sacler in future version.</li>
<li>Only support to configure one metric for basic scale strategy, mutiple metrics will be supported in future version.</li>
<li>scale subresource api for workspace CRD is not covered in this proposal.</li>
<li>The time efficiency of the auto-scaler is not within the scope of this proposal, as it is influenced by mutliple external factors, including GPU node provisioning, LLM image pulling, etc.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="proposal">Proposal<a href="#proposal" class="hash-link" aria-label="Direct link to Proposal" title="Direct link to Proposal">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="auto-scaler-architecture">Auto-Scaler Architecture<a href="#auto-scaler-architecture" class="hash-link" aria-label="Direct link to Auto-Scaler Architecture" title="Direct link to Auto-Scaler Architecture">​</a></h3>
<p>The llm-auto-scaler component scrapes metrics from inference pod according to configurations in LLMAutoScaler CRD, and scaler controller calculate desired replicas by integrating scraped metrics and scale strategy,
then scale workspace replicas through /scale subresource API. The detailed auto-scaler architecture is shown in the following figure:</p>
<p><img loading="lazy" alt="auto-scaler" src="/kaito/docs/assets/images/llm-auto-scaler-86efed50f62bd8986fbd94a0b3004765.png" width="3176" height="1833" class="img_ev3q"></p>
<ul>
<li>LLMAutoScaler CRD: is used as auto-scaler configuration(including scale strategy, target reference, etc.) for specified workspace resource.</li>
<li>Metrics Scraper: a module in auto-scaler of llm-auto-scaler and used for scraping metrics from inference pod.</li>
<li>Scale Strategy: is used to specify scaler, like basic scaler, cron scaler. different scaler have different algorithm to calculate desired replicas. In this proposal, only basic scaler will be supported. and more strategies will be supported in future versions.</li>
<li>Scaler Controller: is used for integrating metrics scraper and scaler strategy, also including invoke scale subresource API of workspace.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="llmautoscaler-crd">LLMAutoScaler CRD<a href="#llmautoscaler-crd" class="hash-link" aria-label="Direct link to LLMAutoScaler CRD" title="Direct link to LLMAutoScaler CRD">​</a></h3>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">type ProtocolType string</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">const (</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	HTTP  ProtocolType = &quot;http&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	HTTPS ProtocolType = &quot;https&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// MetricIdentifier defines the way to fetch the specific metric</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">type MetricIdentifier struct {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	// Name identifies the specific metric to monitor.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	// If unset, vllm:num_requests_waiting will be used.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	Name string</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	// selector is the string-encoded form of a standard kubernetes label selector for the given metric.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	// if unset, a selector related to ScaleTargetRef will be configured(like workspace specified labels).</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	Selector *metav1.LabelSelector</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	// Protocol specify the protocol for accessing pods, http and https are supported.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	// if unset, http will be used.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	Protocol ProtocolType</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	// Port specify the port of pods /metrics endpoint.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	// if unset, 5000 will be used.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	Port string</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">type MetricThreshold struct {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	// High means the uuper threshold, when the value of the monitored metric exceeds this number,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	// the autoscaler will decide to scale up.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	High int32</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	// Low mens the lower threshold. when the value of the monitored metric drops below this number,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	// the autoscaler will scale down.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	Low int32</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">type MetricSource struct {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	// metric identifies the way to fetch target metric</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	// if unset, scaler will fetch metric(vllm:num_requests_waiting) from http://{pod-ip}:5000/metrics endpoint. </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	// and pod-ip is retrieved from pods that related the ScaleTargetRef.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	// +optional</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    Metric MetricIdentifier</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	// threshold defines the boundaries used to trigger scaling actions basd on the monitored metric.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	Threshold MetricThreshold</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">type LLMAutoScalerSpec struct {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	// scaleTargetRef points to the target resource to scale. e.g. Workspace</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	ScaleTargetRef autoscalingv2api.CrossVersionObjectReference</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	// MinReplicas is the lower limit for the number of replicas to which the autoscaler</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	// can scale down. Default value is 1.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	// +optional</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	MinReplicas *int32</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	// MaxReplicas is the upper limit for the number of replicas to which the autoscaler can scale up.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	// It cannot be less that MinReplicas.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	MaxReplicas int32</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	// metrics contains the specifications for how to fetching the specified metric.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	// only one metric is supported currently, and multiple metrics will be supported in future version.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	// this field will be skipped when strategy is cron.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	Metrics []MetricSource</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	// Strategy define which kind of scaler will be used. basic or cron. </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	// In the current version, only basic scaler is supported.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	// If not set, the basic scaler will be selected.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	// +optional</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	Strategy string</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">type LLMAutoScalerStatus struct {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	// lastScaleTime is the last time the LLMAutoScaler scaled the number of inference workloads,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	// used by the autoscaler to control how often the number of inference workloads is changed.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	// +optional</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	LastScaleTime *metav1.Time</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	// currentReplicas is current number of replicas of inference workloads managed by this autoscaler,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	// as last seen by the autoscaler.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	// +optional</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	CurrentReplicas int32</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	// desiredReplicas is the desired number of replicas of inference workloads managed by this autoscaler,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	// as last calculated by the autoscaler.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	DesiredReplicas int32</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	// Conditions is the set of conditions required for this autoscaler to scale its target,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	// and indicates whether or not those conditions are met.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	Conditions []metav1.Condition</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">type LLMAutoScaler struct {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	metav1.TypeMeta</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	metav1.ObjectMeta</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	Spec   LLMAutoScalerSpec</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	Status LLMAutoScalerStatus</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>The details of fields in LLMAutoScaler CRD are described in <code>Metrics Scraper</code> and <code>Baisc Scaler</code>.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="metrics-scraper">Metrics Scraper<a href="#metrics-scraper" class="hash-link" aria-label="Direct link to Metrics Scraper" title="Direct link to Metrics Scraper">​</a></h3>
<ul>
<li>Metrics Scraper fetches specified metrics from pods&#x27; /metrics endpoint according to <code>LLMAutoScaler.Spec.Metrics</code> at 15s interval. only one metric is supported in the first version.</li>
<li>metrics endpoint url: <code>LLMAutoScaler.Spec.Metrics[0].Metric.Protocol://{pod ip}:LLMAutoScaler.Spec.Metrics[0].Metric.Port/metrics</code>, default value is: <code>http://{pod ip}:5000/metrics</code></li>
<li>pod ip: get ip from pods that specified by InferenceAutoScaler.Spec.Metrics[0].Metric.Selector</li>
<li>resolve metric value from response by <code>LLMAutoScaler.Spec.Metrics[0].Metric.Name</code>, and default metric name is: <code>vllm:num_requests_waiting</code></li>
<li>If there are multiple pods are selected, average metric value should be calculated.</li>
<li>If the specified metric can not resolved from the pod, for example, the pod is in the pending state. we should calculate the average value as following rules in order to prevent flapping.<!-- -->
<ul>
<li>In scale up direction: use the 0 as the metric value for missing pods.</li>
<li>In scale down direction: use the <code>LLMAutoScaler.Spec.Metrics[0].Threshold.High</code> as the metric value for missing pods.</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="basic-scaler">Basic Scaler<a href="#basic-scaler" class="hash-link" aria-label="Direct link to Basic Scaler" title="Direct link to Basic Scaler">​</a></h3>
<p>The baisc scaler is used to scale GPU workloads accroding to specified metric changes. The scaling rules are shown as following:</p>
<table><thead><tr><th>item</th><th>scale up</th><th>scale down</th><th>introducation</th></tr></thead><tbody><tr><td>scale step   </td><td>1  </td><td>1  </td><td> only increase/reduce one replica in one scaling action, because the cost of gpu resource is really high </td></tr><tr><td>cooldown seconds   </td><td>600  </td><td>1800  </td><td>a waiting period after a scaling action for preventing frequent scaling</td></tr><tr><td>stablizationwindow seconds   </td><td>0  </td><td>300  </td><td>a lookback window that delays scaling decisions for avoiding premature downscaling</td></tr></tbody></table>
<p>The Scale Strategy Pseudocode</p>
<p>Inputs:</p>
<ul>
<li>CurrentReplicas: Actual number of replicas for target workload, resolved from /scale subresource API.</li>
<li>CurrentWaitingRequests: current waiting requests in inference queue, resolved from pods by metric scraper.</li>
<li>MinReplicas: The max number of replicas for target object, related field: <code>LLMAutoScaler.Spec.MinReplicas</code></li>
<li>MaxReplicas: The max number of replicas for target object, related field: <code>LLMAutoScaler.Spec.MaxReplicas</code></li>
<li>HighThreshold: expected high threshold of waiting requests, related field: <code>LLMAutoScaler.Spec.Metrics[0].Threshold.High</code></li>
<li>LowThreshold: expected low threshold of waiting requests, related field: <code>LLMAutoScaler.Spec.Metrics[0].Threshold.Low</code></li>
<li>ScaleUpStep: the scale step of scaling up action, default value is 1</li>
<li>ScaleDownStep: the scale step of scaling down action, default value is 1</li>
<li>UpStabilizationWindowSeconds: the stabilization window seconds of scaling up action, default value is 0</li>
<li>DownStabilizationWindowSeconds: the stabilization window seconds of scaling down action, default value is 300</li>
<li>UpCoolDownSeconds: the cool down seconds of scaling up action, default value is 600</li>
<li>DownCoolDownSeconds: the cool down seconds of scaling down action, default value is 1800</li>
<li>LastScaleTime: the timestamp for the last scaling action, related field: <code>LLMAutoScaler.Status.LastScaleTime</code></li>
</ul>
<p>Outputs:</p>
<ul>
<li>DesiredReplicas: Desired number of replicas for target workload. and the value will be used for scaling workload through /Scale subresource api.</li>
</ul>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">// 1. calculate the elapsed time for cooldown check</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">coolddownElapsed := now.Sub(LastScaleTime)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// 2. scale up logic</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">if CurrentWaitingRequests &gt; HighThreshold {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	// check stablization window</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	if UpStabilizationWindowSeconds &gt; 0 {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">		windowMetrics := filterMetricsWithinWindow(queueHistory, UpStabilizationWindowSeconds)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">		minInWindow := min(windowMetrics)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">		if minInWindow &lt;= HighThreshold {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">			// maybe it&#x27;s a request spike, so skip scale up</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">			return CurrentReplicas</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">		}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	// check cooldown</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	if coolddownElapsed &gt; UpCoolDownSeconds {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">		return min(CurrentReplicas + ScaleUpStep, MaxReplicas)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// 3. scale down logic</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">if CurrentWaitingRequests &lt; LowThreshold {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	// check stablization window</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	if DownStabilizationWindowSeconds &gt; 0 {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">		windowMetrics := filterMetricsWithinWindow(queueHistory, DownStabilizationWindowSeconds)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">		maxInWindow := max(windowMetrics)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">		if maxInWindow &gt;= LowThreshold {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">			// maybe it&#x27;s a request dip, so skip scale down</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">			return CurrentReplicas</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">		}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	// check cooldown</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	if coolddownElapsed &gt; DownCoolDownSeconds {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">		return max(CurrentReplicas - ScaleDownStep, MinReplicas)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// 4. otherwise, skip scaling action</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">return CurrentReplicas</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="alternatives">Alternatives<a href="#alternatives" class="hash-link" aria-label="Direct link to Alternatives" title="Direct link to Alternatives">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="native-hpa">Native HPA<a href="#native-hpa" class="hash-link" aria-label="Direct link to Native HPA" title="Direct link to Native HPA">​</a></h3>
<p>Native HPA + Prometheus + Prometheus Adapter solution can also be used for scaling inference workloads of Kaito.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="implementation-history">Implementation History<a href="#implementation-history" class="hash-link" aria-label="Direct link to Implementation History" title="Direct link to Implementation History">​</a></h2>
<ul class="contains-task-list containsTaskList_mC6p">
<li class="task-list-item"><input type="checkbox" disabled=""> <!-- -->06/10/2025: Open proposal PR</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/kaito-project/kaito/tree/main/website/docs/proposals/20250620-auto-scaler-for-inference-workloads.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#summary" class="table-of-contents__link toc-highlight">Summary</a></li><li><a href="#motivation" class="table-of-contents__link toc-highlight">Motivation</a><ul><li><a href="#goals" class="table-of-contents__link toc-highlight">Goals</a></li><li><a href="#non-goalsfuture-work" class="table-of-contents__link toc-highlight">Non-Goals/Future Work</a></li></ul></li><li><a href="#proposal" class="table-of-contents__link toc-highlight">Proposal</a><ul><li><a href="#auto-scaler-architecture" class="table-of-contents__link toc-highlight">Auto-Scaler Architecture</a></li><li><a href="#llmautoscaler-crd" class="table-of-contents__link toc-highlight">LLMAutoScaler CRD</a></li><li><a href="#metrics-scraper" class="table-of-contents__link toc-highlight">Metrics Scraper</a></li><li><a href="#basic-scaler" class="table-of-contents__link toc-highlight">Basic Scaler</a></li></ul></li><li><a href="#alternatives" class="table-of-contents__link toc-highlight">Alternatives</a><ul><li><a href="#native-hpa" class="table-of-contents__link toc-highlight">Native HPA</a></li></ul></li><li><a href="#implementation-history" class="table-of-contents__link toc-highlight">Implementation History</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Documentation</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/kaito/docs/">Getting Started</a></li><li class="footer__item"><a class="footer__link-item" href="/kaito/docs/installation">Installation</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/kaito-project/kaito" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://join.slack.com/t/kaito-z6a6575/shared_invite/zt-37gh89vw7-odHfqmPRc5oRnDG99SBJNA" target="_blank" rel="noopener noreferrer" class="footer__link-item">Slack<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Kaito Project, Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>