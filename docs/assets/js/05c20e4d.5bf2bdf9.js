"use strict";(self.webpackChunkkaito_website=self.webpackChunkkaito_website||[]).push([[8],{7730:(e,t,i)=>{i.d(t,{A:()=>n});const n=i.p+"assets/images/workspace-scale-api-ec9f8d5bd28655c5c9c496fe6b7ae52d.png"},8411:(e,t,i)=>{i.d(t,{A:()=>n});const n=i.p+"assets/images/llm-auto-scaler-f431671665e3d9ff19bc2650d7245ac5.png"},8453:(e,t,i)=>{i.d(t,{R:()=>a,x:()=>o});var n=i(6540);const s={},r=n.createContext(s);function a(e){const t=n.useContext(r);return n.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),n.createElement(r.Provider,{value:t},e.children)}},9303:(e,t,i)=>{i.r(t),i.d(t,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>o,toc:()=>l});var n=i(4848),s=i(8453);const r={title:"Auto Scaler For Inference Workloads In Kaito",authors:["@rambohe-ch"],reviewers:["@Fei-Guo","@helayoty","@zhuangqh"],"creation-date":new Date("2025-06-10T00:00:00.000Z"),"last-updated":new Date("2025-06-30T00:00:00.000Z"),status:"provisional","see-also":null},a="Title",o={id:"proposals/auto-scaler-for-inference-workloads",title:"Auto Scaler For Inference Workloads In Kaito",description:"Auto Scaler for inference workloads in Kaito",source:"@site/docs/proposals/20250620-auto-scaler-for-inference-workloads.md",sourceDirName:"proposals",slug:"/proposals/auto-scaler-for-inference-workloads",permalink:"/kaito/docs/proposals/auto-scaler-for-inference-workloads",draft:!1,unlisted:!1,editUrl:"https://github.com/kaito-project/kaito/tree/main/website/docs/proposals/20250620-auto-scaler-for-inference-workloads.md",tags:[],version:"current",sidebarPosition:20250620,frontMatter:{title:"Auto Scaler For Inference Workloads In Kaito",authors:["@rambohe-ch"],reviewers:["@Fei-Guo","@helayoty","@zhuangqh"],"creation-date":"2025-06-10T00:00:00.000Z","last-updated":"2025-06-30T00:00:00.000Z",status:"provisional","see-also":null}},c={},l=[{value:"Summary",id:"summary",level:2},{value:"Motivation",id:"motivation",level:2},{value:"Goals",id:"goals",level:3},{value:"Non-Goals/Future Work",id:"non-goalsfuture-work",level:3},{value:"Proposal",id:"proposal",level:2},{value:"LLM-Auto-Scaler Architecture",id:"llm-auto-scaler-architecture",level:3},{value:"LLMAutoScaler CRD",id:"llmautoscaler-crd",level:3},{value:"Realtime Scaler",id:"realtime-scaler",level:3},{value:"Metrics Scraper",id:"metrics-scraper",level:4},{value:"Scaler Rule",id:"scaler-rule",level:4},{value:"Realtime Scaler Pseudocode",id:"realtime-scaler-pseudocode",level:4},{value:"Cron Scaler",id:"cron-scaler",level:3},{value:"Cron Scaler Pseudocode",id:"cron-scaler-pseudocode",level:4},{value:"Alternatives",id:"alternatives",level:2},{value:"Native HPA",id:"native-hpa",level:3},{value:"Acknowledgments",id:"acknowledgments",level:2},{value:"Implementation History",id:"implementation-history",level:2}];function d(e){const t={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",img:"img",input:"input",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(t.h1,{id:"title",children:"Title"}),"\n",(0,n.jsx)(t.p,{children:"Auto Scaler for inference workloads in Kaito"}),"\n",(0,n.jsx)(t.h2,{id:"summary",children:"Summary"}),"\n",(0,n.jsx)(t.p,{children:"As the number of waiting inference requests increases, it is necessary to scale more inference instances in order to prevent blocking inference requests. On the other hand, if the number of waiting inference requests declines, we should consider reducing inference instances to improve GPU resource utilization.\nNative Kubernetes has provided HPA capability to scale workload instances automatically as the metrics change, but HPA depends on third-party components (like Prometheus, Prometheus Adapter, etc.) to collect custom metrics from the source pods."}),"\n",(0,n.jsx)(t.p,{children:"In this proposal, we hope to support a customized auto-scaler which is specialized for scaling GPU workloads for Kaito. The auto-scaler is designed for a minimalistic configuration experience, with most parameters pre-tuned for optimal performance. This allows users to easily get started without requiring specialized knowledge of LLM."}),"\n",(0,n.jsx)(t.h2,{id:"motivation",children:"Motivation"}),"\n",(0,n.jsxs)(t.p,{children:["LLM inference service is a basic and widely-used feature in Kaito, and Kaito community interest in auto-scaler for inference workloads continues to intensify. Related issues: ",(0,n.jsx)(t.a,{href:"https://github.com/kaito-project/kaito/issues/306",children:"#306"}),", ",(0,n.jsx)(t.a,{href:"https://github.com/kaito-project/kaito/issues/1104",children:"#1104"}),"."]}),"\n",(0,n.jsx)(t.p,{children:"From the technical perspective, it's a good idea to provide auto-scaler capability because the auto-scaler of inference workloads dynamically adjusts the number of inference instances based on request volume\u2014scaling up during traffic spikes to improve inference speed, and scaling down during low demand to minimize GPU resource waste. Furthermore, for workloads with predictable, recurring traffic patterns, a time-based scaler can proactively adjust capacity, ensuring resources are ready before they are needed."}),"\n",(0,n.jsx)(t.p,{children:"The auto-scaler solution for Kaito should be shown as follows:"}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{alt:"auto-scaler",src:i(7730).A+"",width:"3542",height:"2117"})}),"\n",(0,n.jsx)(t.p,{children:"We will divide this auto-scaler feature into two parts as follows:"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsxs)(t.li,{children:["Part one: support scale subresource API for workspace, so different auto-scaler solutions such as KEDA, HPA, etc. can be integrated with Kaito to manage inference workloads dynamically. This part is addressed in another proposal: ",(0,n.jsx)(t.a,{href:"https://github.com/kaito-project/kaito/pull/1184",children:"https://github.com/kaito-project/kaito/pull/1184"}),"."]}),"\n",(0,n.jsx)(t.li,{children:"Part two: support a customized auto-scaler for Kaito. The auto-scaler is designed for a minimalistic configuration experience, with most parameters pre-tuned for optimal performance. This allows users to easily get started without requiring specialized knowledge of LLM. This part will be addressed in this proposal. We name this auto-scaler as llm-auto-scaler, which will support both reactive (metric-based) and proactive (time-based) scaling."}),"\n"]}),"\n",(0,n.jsx)(t.p,{children:"To ensure ease of use, the specialized auto-scaler is hosted in an independent repo (kaito-project/llm-auto-scaler). At the same time, the llm-auto-scaler component can work with Kaito without depending on any third-party components."}),"\n",(0,n.jsx)(t.h3,{id:"goals",children:"Goals"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"llm-auto-scaler is a specialized auto-scaler for scaling gpu workloads automatically, and can integrate with kaito to work."}),"\n",(0,n.jsx)(t.li,{children:"It is flexible enough to support multiple scalers. This proposal introduces two primary scalers: a metric-based scaler for reactive scaling and a cron-based scaler for proactive, scheduled scaling."}),"\n"]}),"\n",(0,n.jsx)(t.h3,{id:"non-goalsfuture-work",children:"Non-Goals/Future Work"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"The time efficiency of the auto-scaler is not within the scope of this proposal, as it is influenced by mutliple external factors, including GPU node provisioning, LLM image pulling, etc."}),"\n",(0,n.jsx)(t.li,{children:"Only support scale vllm workload, and non-vllm is not covered."}),"\n"]}),"\n",(0,n.jsx)(t.h2,{id:"proposal",children:"Proposal"}),"\n",(0,n.jsx)(t.h3,{id:"llm-auto-scaler-architecture",children:"LLM-Auto-Scaler Architecture"}),"\n",(0,n.jsxs)(t.p,{children:["The llm-auto-scaler component determines the desired number of replicas based on the configured scaling strategy in the ",(0,n.jsx)(t.code,{children:"LLMAutoScaler"})," CRD. For metric-based scaling, it scrapes metrics from the inference pods. For time-based scaling, it evaluates cron schedules. The scaler controller then calculates the desired replica count and scales the workspace replicas through the ",(0,n.jsx)(t.code,{children:"/scale"})," subresource API. The detailed auto-scaler architecture is shown in the following figure:"]}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{alt:"auto-scaler",src:i(8411).A+"",width:"4010",height:"2012"})}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.strong,{children:"LLMAutoScaler CRD"}),": Defines the auto-scaler configuration, including the scaling strategy (metric-based or time-based), target resource, and scaling parameters."]}),"\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.strong,{children:"Metrics Scraper"}),": A module of the llm-auto-scaler used for scraping metrics from inference pods. This component is active only when a ",(0,n.jsx)(t.code,{children:"RealtimeScaler"})," (metric-based) strategy is configured."]}),"\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.strong,{children:"Multiple Scalers"}),": The proposal supports two primary scaling strategies: ",(0,n.jsx)(t.code,{children:"RealtimeScaler"})," for reactive scaling based on metrics, and ",(0,n.jsx)(t.code,{children:"CronScaler"})," for proactive scaling based on a predefined schedule. These scalers determine the algorithm used to calculate the desired number of replicas."]}),"\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.strong,{children:"Scaler Controller"}),": The core logic that integrates the selected scale strategy (either with scraped metrics or cron schedules) to calculate the desired replicas and invokes the ",(0,n.jsx)(t.code,{children:"/scale"})," subresource API of the target workspace."]}),"\n"]}),"\n",(0,n.jsx)(t.h3,{id:"llmautoscaler-crd",children:"LLMAutoScaler CRD"}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{children:'type ProtocolType string\n\nconst (\n\tHTTP  ProtocolType = "http"\n\tHTTPS ProtocolType = "https"\n)\n\n// MetricSource defines the way to fetch the specific metric\ntype MetricSource struct {\n\t// Name identifies the specific metric to monitor.\n\t// If unset, vllm:num_requests_waiting will be used.\n\tName string\n\n\t// Protocol specify the protocol for accessing pods, http and https are supported.\n\t// if unset, http will be used.\n\tProtocol ProtocolType\n\n\t// Port specify the port of pods /metrics endpoint.\n\t// if unset, 5000 will be used.\n\tPort string\n\n\t// Path specify the path of the metric endpoint.\n\t// if unset, /metrics will be used.\n\tPath string\n}\n\ntype MetricThreshold struct {\n\t// High means the upper threshold, when the value of the monitored metric exceeds this number,\n\t// the autoscaler will decide to scale up.\n\tHigh int32\n\n\t// Low mens the lower threshold. when the value of the monitored metric drops below this number,\n\t// the autoscaler will scale down.\n\tLow int32\n}\n\ntype Metric struct {\n\t// metric identifies the way to fetch target metric\n\t// if unset, scaler will fetch metric(vllm:num_requests_waiting) from http://{pod-ip}:5000/metrics endpoint. \n\t// and pod-ip is retrieved from pods that related the ScaleTargetRef.\n\t// +optional\n\tSource MetricSource\n\n\t// threshold defines the boundaries used to trigger scaling actions basd on the monitored metric.\n\tThreshold MetricThreshold\n}\n\ntype ScalerRule struct {\n\t// CooldownSeconds defines the waiting period after a scaling action.\n\t// +optional\n\tCooldownSeconds *int32\n\n\t// StabilizationWindowSeconds defines the lookback window to prevent premature scaling.\n\t// +optional\n\tStabilizationWindowSeconds *int32\n\n\t// ScaleStep defines the number of replicas increase/reduce during a scaling action.\n\tScaleStep *int32\n}\n\ntype RealtimeScaler struct {\n\t// Metrics contains the specifications for how to fetch the specified metrics and their scale thresholds.\n\t// If multiple metrics are specified, the scaler will calculate the desired replicas for each metric and choose the highest value.\n\tMetrics []Metric\n\n\t// ScaleUpRule defines the rules for scaling up.\n\t// if unset, ScaleStep will set to 1, CooldownSeconds to 600, and StabilizationWindowSeconds to 30.\n\t// +optional\n\tScaleUpRule ScalerRule\n\n\t// ScaleDownRule defines the rules for scaling down.\n\t// if unset, ScaleStep will set to 1, CooldownSeconds to 1800, and StabilizationWindowSeconds to 300.\n\t// +optional\n\tScaleDownRule ScalerRule\n}\n\ntype CronPolicy struct {\n\t// Name is used to specify the name of the policy.\n\tName string\n\t// The schedule in Cron format, see https://en.wikipedia.org/wiki/Cron.\n\tSchedule string\n\t// TargetReplicas is used to specify the desired number of replicas.\n\tTargetReplicas int32\n\t// TimeZone is the timezone for interpreting the schedule.\n\t// If unset, "Etc/UTC" will be used, which interprets the schedule relative to Coordinated Universal Time.\n\t// +optional\n\tTimeZone string\n}\n\ntype CronScaler struct {\n\t// Policies is a list of cron policies that define the cron scaling schedule.\n\tPolicies []CronPolicy\n}\n\ntype Scaler struct {\n\t// Realtime represents a realtime auto scaler that scales up/down workloads according to metric changes.\n\t// At the same time, realtime auto scaler works in a passive mode, it means inference requests will be blocked before new inference workloads added.\n\t// +optional\n\tRealtime *RealtimeScaler\n\n\t// Cron represents a cron auto scaler that scales up/down workloads at specified time.\n\t// And cron auto scaler works in a proactive mode, the inference workloads will be ready before the inference requests spikes.\n\tCron *CronScaler\n}\n\ntype LLMAutoScalerSpec struct {\n\t// scaleTargetRef points to the target resource to scale. e.g. Workspace\n\tScaleTargetRef autoscalingv2api.CrossVersionObjectReference\n\n\t// MinReplicas is the lower limit for the number of replicas to which the autoscaler\n\t// can scale down. Default value is 1.\n\t// +optional\n\tMinReplicas *int32\n\n\t// MaxReplicas is the upper limit for the number of replicas to which the autoscaler can scale up.\n\t// It cannot be less that MinReplicas.\n\tMaxReplicas int32\n\n\t// Scaler defines the scaling strategy, which can be either metric-based (Realtime) or time-based (Cron).\n\tScaler Scaler\n}\n\ntype LLMAutoScalerStatus struct {\n\t// lastScaleTime is the last time the LLMAutoScaler scaled the number of inference workloads,\n\t// used by the autoscaler to control how often the number of inference workloads is changed.\n\t// +optional\n\tLastScaleTime *metav1.Time\n\n\t// currentReplicas is current number of replicas of inference workloads managed by this autoscaler,\n\t// as last seen by the autoscaler.\n\t// +optional\n\tCurrentReplicas int32\n\n\t// desiredReplicas is the desired number of replicas of inference workloads managed by this autoscaler,\n\t// as last calculated by the autoscaler.\n\tDesiredReplicas int32\n\n\t// Conditions is the set of conditions required for this autoscaler to scale its target,\n\t// and indicates whether or not those conditions are met.\n\tConditions []metav1.Condition\n}\n\ntype LLMAutoScaler struct {\n\tmetav1.TypeMeta\n\tmetav1.ObjectMeta\n\n\tSpec   LLMAutoScalerSpec\n\tStatus LLMAutoScalerStatus\n}\n'})}),"\n",(0,n.jsx)(t.h3,{id:"realtime-scaler",children:"Realtime Scaler"}),"\n",(0,n.jsx)(t.h4,{id:"metrics-scraper",children:"Metrics Scraper"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsxs)(t.li,{children:["Metrics Scraper fetches specified metrics from pods' /metrics endpoint according to ",(0,n.jsx)(t.code,{children:"LLMAutoScaler.Spec.Scaler.Realtime.Metrics"})," at a 15s interval."]}),"\n",(0,n.jsxs)(t.li,{children:["For each metric in the ",(0,n.jsx)(t.code,{children:"Metrics"})," list, the scraper will:","\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsxs)(t.li,{children:["Construct the endpoint URL: ",(0,n.jsx)(t.code,{children:"metric.Source.Protocol://{pod ip}:metric.Source.Port/metric.Source.Path"}),". The default is ",(0,n.jsx)(t.code,{children:"http://{pod ip}:5000/metrics"}),"."]}),"\n",(0,n.jsxs)(t.li,{children:["Get pod IPs from the pods referenced by ",(0,n.jsx)(t.code,{children:"InferenceAutoScaler.Spec.ScaleTargetRef"}),"."]}),"\n",(0,n.jsxs)(t.li,{children:["Resolve the metric value from the response using ",(0,n.jsx)(t.code,{children:"metric.Source.Name"}),". The default metric name is ",(0,n.jsx)(t.code,{children:"vllm:num_requests_waiting"}),"."]}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(t.li,{children:"If multiple pods are selected, the average metric value will be calculated for each metric."}),"\n",(0,n.jsxs)(t.li,{children:["If a metric cannot be resolved from a pod (e.g., the pod is pending), the average value will be calculated using the following rules to prevent flapping:","\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"In the scale-up direction: use 0 as the metric value for missing pods."}),"\n",(0,n.jsxs)(t.li,{children:["In the scale-down direction: use the metric's ",(0,n.jsx)(t.code,{children:"High"})," threshold as the value for missing pods."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(t.h4,{id:"scaler-rule",children:"Scaler Rule"}),"\n",(0,n.jsx)(t.p,{children:"The realtime scaler is used to scale GPU workloads according to specified metric changes. it means that realtime scaler is a passive scaling.\nand the scaling rules will be integrated to calculate desired replicas."}),"\n",(0,n.jsxs)(t.table,{children:[(0,n.jsx)(t.thead,{children:(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.th,{children:"item"}),(0,n.jsx)(t.th,{children:"scale up(default value)"}),(0,n.jsx)(t.th,{children:"scale down(default value)"}),(0,n.jsx)(t.th,{children:"introduication"})]})}),(0,n.jsxs)(t.tbody,{children:[(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"scale step\xa0\xa0\xa0"}),(0,n.jsx)(t.td,{children:"1\xa0\xa0"}),(0,n.jsx)(t.td,{children:"1\xa0\xa0"}),(0,n.jsx)(t.td,{children:"\xa0only increase/reduce one replica in a scaling action, because the cost of gpu resource is really high\xa0"})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"cooldown seconds\xa0\xa0\xa0"}),(0,n.jsx)(t.td,{children:"600\xa0\xa0"}),(0,n.jsx)(t.td,{children:"1800\xa0\xa0"}),(0,n.jsx)(t.td,{children:"a waiting period after a scaling action for preventing frequent scaling"})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"stablizationwindow seconds\xa0\xa0\xa0"}),(0,n.jsx)(t.td,{children:"30\xa0\xa0"}),(0,n.jsx)(t.td,{children:"300\xa0\xa0"}),(0,n.jsx)(t.td,{children:"a lookback window that delays scaling decisions for avoiding premature scaling. and we hope scale-up to respond more quickly, while scale-down should occur more slowly. It means that scale-up is triggered only when the metric value exceeds the high threshold in 2 consecutive periods. and scale-down is triggered only when the metric value less than low threshold in 20 consecutive periods"})]})]})]}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"Because cron scaler is used to scale workloads at scheduled time, so scaler rule will be skipped by cron scaler."}),"\n"]}),"\n",(0,n.jsx)(t.h4,{id:"realtime-scaler-pseudocode",children:"Realtime Scaler Pseudocode"}),"\n",(0,n.jsx)(t.p,{children:"Inputs:"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"CurrentReplicas: Actual number of replicas for target workload, resolved from /scale subresource API."}),"\n",(0,n.jsx)(t.li,{children:"AllMetrics: A list of all metrics with their current values, resolved by the metric scraper."}),"\n",(0,n.jsxs)(t.li,{children:["MinReplicas: The min number of replicas for target object, related field: ",(0,n.jsx)(t.code,{children:"LLMAutoScaler.Spec.MinReplicas"})]}),"\n",(0,n.jsxs)(t.li,{children:["MaxReplicas: The max number of replicas for target object, related field: ",(0,n.jsx)(t.code,{children:"LLMAutoScaler.Spec.MaxReplicas"})]}),"\n",(0,n.jsxs)(t.li,{children:["ScaleUpStep: the scale step of scaling up action, related field: ",(0,n.jsx)(t.code,{children:"LLMAutoScaler.Spec.Scaler.Realtime.ScaleUpRule.ScaleStep"})]}),"\n",(0,n.jsxs)(t.li,{children:["ScaleDownStep: the scale step of scaling down action, related field: ",(0,n.jsx)(t.code,{children:"LLMAutoScaler.Spec.Scaler.Realtime.ScaleDownRule.ScaleStep"})]}),"\n",(0,n.jsxs)(t.li,{children:["UpStabilizationWindowSeconds: the stabilization window seconds of scaling up action, related field: ",(0,n.jsx)(t.code,{children:"LLMAutoScaler.Spec.Scaler.Realtime.ScaleUpRule.StabilizationWindowSeconds"})]}),"\n",(0,n.jsxs)(t.li,{children:["DownStabilizationWindowSeconds: the stabilization window seconds of scaling down action, related field: ",(0,n.jsx)(t.code,{children:"LLMAutoScaler.Spec.Scaler.Realtime.ScaleDownRule.StabilizationWindowSeconds"})]}),"\n",(0,n.jsxs)(t.li,{children:["UpCoolDownSeconds: the cool down seconds of scaling up action, related field: ",(0,n.jsx)(t.code,{children:"LLMAutoScaler.Spec.Scaler.Realtime.ScaleUpRule.CoolDownSeconds"})]}),"\n",(0,n.jsxs)(t.li,{children:["DownCoolDownSeconds: the cool down seconds of scaling down action, related field: ",(0,n.jsx)(t.code,{children:"LLMAutoScaler.Spec.Scaler.Realtime.ScaleDownRule.CoolDownSeconds"})]}),"\n",(0,n.jsxs)(t.li,{children:["LastScaleTime: the timestamp for the last scaling action, related field: ",(0,n.jsx)(t.code,{children:"LLMAutoScaler.Status.LastScaleTime"})]}),"\n",(0,n.jsx)(t.li,{children:"MetricsHistory: include all latest metric values for each autoscaler, the type is: map[string][]timestampedMetricValue"}),"\n"]}),"\n",(0,n.jsx)(t.p,{children:"Outputs:"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"DesiredReplicas: Desired number of replicas for target workload. and the value will be used for scaling workload through /Scale subresource api."}),"\n"]}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-go",children:"type timestampedMetricValue struct {\n\tvalue int32\n\ttimestamp time.Time\n}\n\nfunc calculateDesiredReplicas(INPUTS) OUTPUTS{\n\tdesiredReplicas := CurrentReplicas\n\t\n\t// Iterate over all metrics and calculate the desired replicas for each.\n\t// The final desired replicas will be the maximum of all calculations.\n\tfor _, metric := range AllMetrics {\n\t\t// The logic inside calculateReplicasForMetric is the same as the previous single-metric pseudocode.\n\t\t// It returns a calculated number of replicas based on one metric's value and thresholds.\n\t\tmetricDesiredReplicas := calculateReplicasForMetric(metric, CurrentReplicas, INPUTS)\n\t\tif metricDesiredReplicas > desiredReplicas {\n\t\t\tdesiredReplicas = metricDesiredReplicas\n\t\t}\n\t}\n\n\treturn desiredReplicas\n}\n\n\nfunc calculateReplicasForMetric(metric, CurrentReplicas, INPUTS) OUTPUTS{\n\t// 0. ensure metrics history updated for each reconcile\n\tdefer updateMetricsHistory(LLMAutoScaler.Name + metric.Name, metric.Value, MetricsHistory, UpStabilizationWindowSeconds, DownStabilizationWindowSeconds)\n\n\t// 1. calculate the elapsed time for cooldown check\n\tcooldownElapsed := time.Now().Sub(LastScaleTime)\n\n\t// 2. scale up logic\n\tif metric.Value > metric.Threshold.High {\n\t\t// check stablization window\n\t\tif UpStabilizationWindowSeconds > 0 {\n\t\t\twindowMetrics := filterMetricsWithinWindow(LLMAutoScaler.Name + metric.Name, MetricsHistory, UpStabilizationWindowSeconds)\n\t\t\tminInWindow := min(windowMetrics)\n\t\t\tif minInWindow <= metric.Threshold.High {\n\t\t\t\t// there is a metric value below high threshold, so skip scale up\n\t\t\t\treturn CurrentReplicas\n\t\t\t}\n\t\t}\n\n\t\t// check cooldown\n\t\tif cooldownElapsed > UpCoolDownSeconds {\n\t\t\treturn min(CurrentReplicas + ScaleUpStep, MaxReplicas)\n\t\t}\n\t}\n\n\t// 3. scale down logic\n\tif metric.Value < metric.Threshold.Low {\n\t\t// check stablization window\n\t\tif DownStabilizationWindowSeconds > 0 {\n\t\t\twindowMetrics := filterMetricsWithinWindow(LLMAutoScaler.Name + metric.Name, MetricsHistory, DownStabilizationWindowSeconds)\n\t\t\tmaxInWindow := max(windowMetrics)\n\t\t\tif maxInWindow >= metric.Threshold.Low {\n\t\t\t\t// there is a metric value above low threshold, so skip scale down\n\t\t\t\treturn CurrentReplicas\n\t\t\t}\n\t\t}\n\n\t\t// check cooldown\n\t\tif cooldownElapsed > DownCoolDownSeconds {\n\t\t\treturn max(CurrentReplicas - ScaleDownStep, MinReplicas)\n\t\t}\n\t}\n\n\t// 4. otherwise, skip scaling action\n\treturn CurrentReplicas\n}\n\nfunc filterMetricsWithinWindow(key, metricsHistory, stabilizationWindowSeconds) windowMetrics {\n\twindowStartTime := time.Now().Add(-time.Second * time.Duration(stabilizationWindowSeconds))\n\n\tfor i, timestampMetric := range metricsHistory[key] {\n\t\tif timestampMetric.timestamp.After(windowStartTime) {\n\t\t\twindowMetrics = append(windowMetrics, timestampMetric.Value)\n\t\t}\n\t}\n\n\treturn windowMetrics\n}\n\nfunc updateMetricsHistory(key, currentValue, metricsHistory, upStabilizationWindowSeconds, downStabilizationWindowSeconds) {\n\tupWindowStartTime := time.Now().Add(-time.Second * time.Duration(upStabilizationWindowSeconds))\n\tdownWindowStartTime := time.Now().Add(-time.Second * time.Duration(downStabilizationWindowSeconds))\n\n\tfoundStaleMetric := false\n\tstaleMetricIndex := 0\n\tfor i, timestampMetric := range metricsHistory[key] {\n\t\tif timestampMetric.timestamp.Before(upWindowStartTime) && timestampMetric.timestamp.Before(downWindowStartTime) {\n\t\t\tfoundStaleMetric = true\n\t\t\tstaleMetricIndex = i\n\t\t}\n\t}\n\n\t// use the stale metric slot for controlling the size of metric slice\n\tif foundStaleMetric {\n\t\tmetricsHistory[key][staleMetricIndex] = timestampedMetricValue{currentValue, time.Now()}\n\t} else {\n\t\tmetricsHistory[key] = append(metricsHistory[key], timestampedMetricValue{currentValue, time.Now())\n\t}\n}\n"})}),"\n",(0,n.jsx)(t.h3,{id:"cron-scaler",children:"Cron Scaler"}),"\n",(0,n.jsx)(t.p,{children:"The Cron Scaler provides a proactive, time-based scaling mechanism that adjusts the number of replicas at scheduled times. Unlike the Realtime Scaler, which reacts to metric changes, the Cron Scaler allows you to align resource allocation with predictable traffic patterns, such as scaling up before business hours and scaling down during nights or weekends. This approach ensures that capacity is available before demand spikes, improving responsiveness and user experience."}),"\n",(0,n.jsxs)(t.p,{children:["Each cron policy defines a ",(0,n.jsx)(t.code,{children:"schedule"})," in standard cron format and a ",(0,n.jsx)(t.code,{children:"targetReplicas"})," count. The autoscaler will continuously monitor the clock and, when the current time matches a policy's schedule, it will adjust the workload's replica count to the specified target."]}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"Take the following policies as an example:"}),"\n"]}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-yaml",children:'policies:\n  # The "scale-up" policy increases the number of replicas to 5 at 8:00 AM every day\n  # to handle peak traffic during business hours.\n  - name: "scale-up"\n    schedule: "0 8 * * *" # Cron schedule for 8:00 AM daily\n    targetReplicas: 5\n    timeZone: "America/New_York" # Uses Eastern Time\n  # The "scale-down" policy reduces the number of replicas to 1 at 1 minute past midnight (00:01)\n  # every day to conserve resources during off-peak hours.\n  - name: "scale-down"\n    schedule: "1 0 * * *" # Cron schedule for 00:01 AM daily\n    targetReplicas: 1\n    timeZone: "America/New_York" # Uses Eastern Time\n'})}),"\n",(0,n.jsxs)(t.p,{children:["The ",(0,n.jsx)(t.code,{children:"scale-up"})," policy will scale the number of replicas to 5 at 08:00 every day in the ",(0,n.jsx)(t.code,{children:"America/New_York"})," timezone, and the ",(0,n.jsx)(t.code,{children:"scale-down"})," policy will scale the number of replicas to 1 at 00:01 every day in the ",(0,n.jsx)(t.code,{children:"America/New_York"})," timezone."]}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"TimeZone Support"}),": The ",(0,n.jsx)(t.code,{children:"timeZone"}),' field allows you to specify the timezone for interpreting the cron schedule. This is particularly useful for aligning scaling actions with business hours in specific geographic regions. The timezone must be a valid IANA Time Zone identifier (e.g., "America/New_York", "Europe/London", "Asia/Tokyo"). If not specified, it defaults to "Etc/UTC". More information can be found in ',(0,n.jsx)(t.a,{href:"https://en.wikipedia.org/wiki/List_of_tz_database_time_zones",children:"https://en.wikipedia.org/wiki/List_of_tz_database_time_zones"})]}),"\n",(0,n.jsx)(t.h4,{id:"cron-scaler-pseudocode",children:"Cron Scaler Pseudocode"}),"\n",(0,n.jsx)(t.p,{children:"Inputs:"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"CurrentReplicas: Actual number of replicas for the target workload."}),"\n",(0,n.jsxs)(t.li,{children:["CronPolicies: The list of cron policies defined in ",(0,n.jsx)(t.code,{children:"LLMAutoScaler.Spec.Scaler.Cron.Policies"}),"."]}),"\n",(0,n.jsx)(t.li,{children:"MinReplicas: The minimum number of replicas for the target object."}),"\n",(0,n.jsx)(t.li,{children:"MaxReplicas: The maximum number of replicas for the target object."}),"\n"]}),"\n",(0,n.jsx)(t.p,{children:"Outputs:"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"DesiredReplicas: Desired number of replicas for the target workload."}),"\n"]}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-go",children:'import (\n\t"time"\n\t"github.com/robfig/cron/v3"\n)\n\nfunc calculateDesiredReplicasCron(INPUTS) OUTPUTS {\n\tvar matchingReplicas []int32\n\n\t// Find all policies that match the current time\n\tfor _, policy := range CronPolicies {\n\t\tlocation, err := time.LoadLocation(policy.TimeZone)\n\t\tif err != nil {\n\t\t\tlocation = time.UTC\n\t\t}\n\t\tnowInLocation := time.Now().In(location)\n\n\t\tschedule, err := cron.ParseStandard(policy.Schedule)\n\t\tif err != nil {\n\t\t\tcontinue\n\t\t}\n\n\t\t// Check if the last run was within the last minute, considering the policy\'s timezone\n\t\tlastRun := schedule.Next(nowInLocation.Add(-time.Minute))\n\t\tif nowInLocation.Truncate(time.Minute).Equal(lastRun.Truncate(time.Minute)) {\n\t\t\tmatchingReplicas = append(matchingReplicas, policy.TargetReplicas)\n\t\t}\n\t}\n\n\t// If no policy matches, no change in replicas\n\tif len(matchingReplicas) == 0 {\n\t\treturn CurrentReplicas\n\t}\n\n\t// If multiple policies match, pick the one with the highest replica count\n\t// to ensure capacity for the highest-demand schedule.\n\tdesired := matchingReplicas[0]\n\tfor i := 1; i < len(matchingReplicas); i++ {\n\t\tif matchingReplicas[i] > desired {\n\t\t\tdesired = matchingReplicas[i]\n\t\t}\n\t}\n\n\t// Clamp the desired replicas within the min/max boundaries\n\tif desired < MinReplicas {\n\t\tdesired = MinReplicas\n\t}\n\tif desired > MaxReplicas {\n\t\tdesired = MaxReplicas\n\t}\n\n\treturn desired\n}\n'})}),"\n",(0,n.jsx)(t.h2,{id:"alternatives",children:"Alternatives"}),"\n",(0,n.jsx)(t.h3,{id:"native-hpa",children:"Native HPA"}),"\n",(0,n.jsx)(t.p,{children:"Native HPA + Prometheus + Prometheus Adapter solution can also be used for scaling inference workloads of Kaito."}),"\n",(0,n.jsx)(t.h2,{id:"acknowledgments",children:"Acknowledgments"}),"\n",(0,n.jsxs)(t.p,{children:["We would like to acknowledge the Kubernetes ",(0,n.jsx)(t.a,{href:"https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/",children:"Horizontal Pod Autoscaler (HPA)"}),", ",(0,n.jsx)(t.a,{href:"https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/",children:"CronJob"})," and ",(0,n.jsx)(t.a,{href:"https://github.com/vllm-project/aibrix",children:"AIBrix"})," for providing inspiration and reference points in designing the LLMAutoScaler API. Their approaches to scaling workloads dynamically based on metrics have been instrumental in shaping this proposal."]}),"\n",(0,n.jsx)(t.h2,{id:"implementation-history",children:"Implementation History"}),"\n",(0,n.jsxs)(t.ul,{className:"contains-task-list",children:["\n",(0,n.jsxs)(t.li,{className:"task-list-item",children:[(0,n.jsx)(t.input,{type:"checkbox",disabled:!0})," ","06/10/2025: Open proposal PR"]}),"\n"]})]})}function h(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(d,{...e})}):d(e)}}}]);