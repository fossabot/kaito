"use strict";(self.webpackChunkkaito_website=self.webpackChunkkaito_website||[]).push([[8],{7730:(e,t,i)=>{i.d(t,{A:()=>n});const n=i.p+"assets/images/workspace-scale-api-ec9f8d5bd28655c5c9c496fe6b7ae52d.png"},8411:(e,t,i)=>{i.d(t,{A:()=>n});const n=i.p+"assets/images/llm-auto-scaler-86efed50f62bd8986fbd94a0b3004765.png"},8453:(e,t,i)=>{i.d(t,{R:()=>o,x:()=>a});var n=i(6540);const s={},r=n.createContext(s);function o(e){const t=n.useContext(r);return n.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function a(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),n.createElement(r.Provider,{value:t},e.children)}},9303:(e,t,i)=>{i.r(t),i.d(t,{assets:()=>c,contentTitle:()=>o,default:()=>u,frontMatter:()=>r,metadata:()=>a,toc:()=>l});var n=i(4848),s=i(8453);const r={title:"Auto Scaler For Inference Workloads In Kaito",authors:["@rambohe-ch"],reviewers:["@Fei-Guo","@helayoty","@zhuangqh"],"creation-date":new Date("2025-06-10T00:00:00.000Z"),"last-updated":new Date("2025-06-20T00:00:00.000Z"),status:"provisional","see-also":null},o="Title",a={id:"proposals/auto-scaler-for-inference-workloads",title:"Auto Scaler For Inference Workloads In Kaito",description:"Auto Scaler for inference workloads in Kaito",source:"@site/docs/proposals/20250620-auto-scaler-for-inference-workloads.md",sourceDirName:"proposals",slug:"/proposals/auto-scaler-for-inference-workloads",permalink:"/kaito/docs/proposals/auto-scaler-for-inference-workloads",draft:!1,unlisted:!1,editUrl:"https://github.com/kaito-project/kaito/tree/main/website/docs/proposals/20250620-auto-scaler-for-inference-workloads.md",tags:[],version:"current",sidebarPosition:20250620,frontMatter:{title:"Auto Scaler For Inference Workloads In Kaito",authors:["@rambohe-ch"],reviewers:["@Fei-Guo","@helayoty","@zhuangqh"],"creation-date":"2025-06-10T00:00:00.000Z","last-updated":"2025-06-20T00:00:00.000Z",status:"provisional","see-also":null}},c={},l=[{value:"Summary",id:"summary",level:2},{value:"Motivation",id:"motivation",level:2},{value:"Goals",id:"goals",level:3},{value:"Non-Goals/Future Work",id:"non-goalsfuture-work",level:3},{value:"Proposal",id:"proposal",level:2},{value:"Auto-Scaler Architecture",id:"auto-scaler-architecture",level:3},{value:"LLMAutoScaler CRD",id:"llmautoscaler-crd",level:3},{value:"Metrics Scraper",id:"metrics-scraper",level:3},{value:"Basic Scaler",id:"basic-scaler",level:3},{value:"Alternatives",id:"alternatives",level:2},{value:"Native HPA",id:"native-hpa",level:3},{value:"Implementation History",id:"implementation-history",level:2}];function d(e){const t={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",img:"img",input:"input",li:"li",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(t.h1,{id:"title",children:"Title"}),"\n",(0,n.jsx)(t.p,{children:"Auto Scaler for inference workloads in Kaito"}),"\n",(0,n.jsx)(t.h2,{id:"summary",children:"Summary"}),"\n",(0,n.jsx)(t.p,{children:"As the number of waiting inference requests increase, It is necessary to scale more inference instances in order to preventing to block inference requests. on the other hand, If the number of waiting inference requests declines, we should consider to reduce inference instances for improving gpu resource utilization.\nNative Kubernetes has provided HPA capability to scale workload instance automatically as the metrics change. but HPA depends the third-party components(like prometheus, prometheus-adapter, etc.) to collect custom metrics from the source pods."}),"\n",(0,n.jsx)(t.p,{children:"In this proposal, we hope to support a customized auto-sacler which is specialized for scaling GPU worklods for kaito. The auto-scaler is designed for a minimalistic configuration experience, with most parameters pre-tuned for optimal performance. This allows users to easily get started without requiring specialized knowledge of LLM."}),"\n",(0,n.jsx)(t.p,{children:"by the way, scale subresource api for workspace CRD is addressed in this proposal:"}),"\n",(0,n.jsx)(t.h2,{id:"motivation",children:"Motivation"}),"\n",(0,n.jsxs)(t.p,{children:["LLM inference service is a baisc and widly-used feature in Kaito, and Kaito community interest in auto scaler for inference workloads continues to intensify, related issues: ",(0,n.jsx)(t.a,{href:"https://github.com/kaito-project/kaito/issues/306",children:"#306"}),", ",(0,n.jsx)(t.a,{href:"https://github.com/kaito-project/kaito/issues/1104",children:"#1104"}),"."]}),"\n",(0,n.jsx)(t.p,{children:"From the technical perspective, It's a good idea to provide auto-scaler capability, becasue the auto-scaler of inference workloads dynamically adjusts the number of inference instances based on request volume--scaling up during traffic spikes to improve inference speed, and scaling down during low demand to minimize GPU resource waste."}),"\n",(0,n.jsx)(t.p,{children:"The auto-scaler solution for kaito should be shown as following:"}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{alt:"auto-scaler",src:i(7730).A+"",width:"3542",height:"2117"})}),"\n",(0,n.jsx)(t.p,{children:"and we will divide this auto-scaler feature into two parts as following:"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsxs)(t.li,{children:["Part one: support scale subresource api for workspace, so different auto-scaler solutions such as KEDA, HPA, etc. can be integrated with Kaito to mamage inference workloads dynamically. This part is addressed in another proposal: ",(0,n.jsx)(t.a,{href:"https://github.com/kaito-project/kaito/pull/1184",children:"https://github.com/kaito-project/kaito/pull/1184"}),"."]}),"\n",(0,n.jsx)(t.li,{children:"Part two: support a customized auto-sacler for kaito. The auto-scaler is designed for a minimalistic configuration experience, with most parameters pre-tuned for optimal performance. This allows users to easily get started without requiring specialized knowledge of LLM. This part will be addressed in this proposal. we name this auto-scaler as llm-auto-scaler."}),"\n"]}),"\n",(0,n.jsx)(t.p,{children:"To ensure ease of use, The specialized auto-scaler is hosted in a independent repo(kaito-project/llm-auto-scaler). at the same time, the llm-auto-scaler component can work with kaito without depending on any third-party components."}),"\n",(0,n.jsx)(t.h3,{id:"goals",children:"Goals"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"llm-auto-scaler is a specialized auto-scaler for scaling gpu workloads automatically, and can integrate with kaito to work."}),"\n",(0,n.jsx)(t.li,{children:"It is flexible to support mulitple scale strategies, and only one basic scale strategy(scaling workloads according to metrics change) is supported in the first version."}),"\n"]}),"\n",(0,n.jsx)(t.h3,{id:"non-goalsfuture-work",children:"Non-Goals/Future Work"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"Support cron scale strategy(like cron job) for llm-auto-sacler in future version."}),"\n",(0,n.jsx)(t.li,{children:"Only support to configure one metric for basic scale strategy, mutiple metrics will be supported in future version."}),"\n",(0,n.jsx)(t.li,{children:"The time efficiency of the auto-scaler is not within the scope of this proposal, as it is influenced by mutliple external factors, including GPU node provisioning, LLM image pulling, etc."}),"\n",(0,n.jsx)(t.li,{children:"Only support scale vllm workload, and non-vllm is not covered."}),"\n"]}),"\n",(0,n.jsx)(t.h2,{id:"proposal",children:"Proposal"}),"\n",(0,n.jsx)(t.h3,{id:"auto-scaler-architecture",children:"Auto-Scaler Architecture"}),"\n",(0,n.jsx)(t.p,{children:"The llm-auto-scaler component scrapes metrics from inference pod according to configurations in LLMAutoScaler CRD, and scaler controller calculate desired replicas by integrating scraped metrics and scale strategy,\nthen scale workspace replicas through /scale subresource API. The detailed auto-scaler architecture is shown in the following figure:"}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{alt:"auto-scaler",src:i(8411).A+"",width:"3176",height:"1833"})}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"LLMAutoScaler CRD: is used as auto-scaler configuration(including scale strategy, target reference, etc.) for specified workspace resource."}),"\n",(0,n.jsx)(t.li,{children:"Metrics Scraper: a module of llm-auto-scaler and used for scraping metrics from inference pods."}),"\n",(0,n.jsx)(t.li,{children:"Scale Strategy: is used to specify scaler, like basic scaler, cron scaler. different scaler have different algorithm to calculate desired replicas. In this proposal, only basic scaler will be supported. and more strategies will be supported in future versions."}),"\n",(0,n.jsx)(t.li,{children:"Scaler Controller: is used for integrating metrics scraper and scaler strategy, also including invoke scale subresource API of workspace."}),"\n"]}),"\n",(0,n.jsx)(t.h3,{id:"llmautoscaler-crd",children:"LLMAutoScaler CRD"}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{children:'type ProtocolType string\n\nconst (\n\tHTTP  ProtocolType = "http"\n\tHTTPS ProtocolType = "https"\n)\n\n// MetricSource defines the way to fetch the specific metric\ntype MetricSource struct {\n\t// Name identifies the specific metric to monitor.\n\t// If unset, vllm:num_requests_waiting will be used.\n\tName string\n\n\t// Protocol specify the protocol for accessing pods, http and https are supported.\n\t// if unset, http will be used.\n\tProtocol ProtocolType\n\n\t// Port specify the port of pods /metrics endpoint.\n\t// if unset, 5000 will be used.\n\tPort string\n\n\t// Path specify the path of the metric endpoint.\n\t// if unset, /metrics will be used.\n\tPath string\n}\n\ntype MetricThreshold struct {\n\t// High means the upper threshold, when the value of the monitored metric exceeds this number,\n\t// the autoscaler will decide to scale up.\n\tHigh int32\n\n\t// Low mens the lower threshold. when the value of the monitored metric drops below this number,\n\t// the autoscaler will scale down.\n\tLow int32\n}\n\ntype Metric struct {\n\t// metric identifies the way to fetch target metric\n\t// if unset, scaler will fetch metric(vllm:num_requests_waiting) from http://{pod-ip}:5000/metrics endpoint. \n\t// and pod-ip is retrieved from pods that related the ScaleTargetRef.\n\t// +optional\n    Source MetricSource\n\n\t// threshold defines the boundaries used to trigger scaling actions basd on the monitored metric.\n\tThreshold MetricThreshold\n}\n\ntype LLMAutoScalerSpec struct {\n\t// scaleTargetRef points to the target resource to scale. e.g. Workspace\n\tScaleTargetRef autoscalingv2api.CrossVersionObjectReference\n\n\t// MinReplicas is the lower limit for the number of replicas to which the autoscaler\n\t// can scale down. Default value is 1.\n\t// +optional\n\tMinReplicas *int32\n\n\t// MaxReplicas is the upper limit for the number of replicas to which the autoscaler can scale up.\n\t// It cannot be less that MinReplicas.\n\tMaxReplicas int32\n\n\t// metrics contains the specifications for how to fetching the specified metric and scale threshold of metric.\n\t// only one metric is supported currently, and multiple metrics will be supported in future version.\n\t// this field will be skipped when strategy is cron.\n\tMetrics []Metric\n\n\t// Strategy define which kind of scaler will be used. basic or cron. \n\t// In the current version, only basic scaler is supported.\n\t// If not set, the basic scaler will be selected.\n\t// +optional\n\tStrategy string\n}\n\ntype LLMAutoScalerStatus struct {\n\t// lastScaleTime is the last time the LLMAutoScaler scaled the number of inference workloads,\n\t// used by the autoscaler to control how often the number of inference workloads is changed.\n\t// +optional\n\tLastScaleTime *metav1.Time\n\n\t// currentReplicas is current number of replicas of inference workloads managed by this autoscaler,\n\t// as last seen by the autoscaler.\n\t// +optional\n\tCurrentReplicas int32\n\n\t// desiredReplicas is the desired number of replicas of inference workloads managed by this autoscaler,\n\t// as last calculated by the autoscaler.\n\tDesiredReplicas int32\n\n\t// Conditions is the set of conditions required for this autoscaler to scale its target,\n\t// and indicates whether or not those conditions are met.\n\tConditions []metav1.Condition\n}\n\ntype LLMAutoScaler struct {\n\tmetav1.TypeMeta\n\tmetav1.ObjectMeta\n\n\tSpec   LLMAutoScalerSpec\n\tStatus LLMAutoScalerStatus\n}\n'})}),"\n",(0,n.jsxs)(t.p,{children:["The details of fields in LLMAutoScaler CRD are described in ",(0,n.jsx)(t.code,{children:"Metrics Scraper"})," and ",(0,n.jsx)(t.code,{children:"Baisc Scaler"}),"."]}),"\n",(0,n.jsx)(t.h3,{id:"metrics-scraper",children:"Metrics Scraper"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsxs)(t.li,{children:["Metrics Scraper fetches specified metrics from pods' /metrics endpoint according to ",(0,n.jsx)(t.code,{children:"LLMAutoScaler.Spec.Metrics"})," at 15s interval. only one metric is supported in the first version."]}),"\n",(0,n.jsxs)(t.li,{children:["metrics endpoint url: ",(0,n.jsx)(t.code,{children:"LLMAutoScaler.Spec.Metrics[0].Source.Protocol://{pod ip}:LLMAutoScaler.Spec.Metrics[0].Source.Port/metrics"}),", default value is: ",(0,n.jsx)(t.code,{children:"http://{pod ip}:5000/metrics"})]}),"\n",(0,n.jsxs)(t.li,{children:["pod ip: get ip from pods that referenced by ",(0,n.jsx)(t.code,{children:"InferenceAutoScaler.Spec.ScaleTargetRef"})]}),"\n",(0,n.jsxs)(t.li,{children:["resolve metric value from response by ",(0,n.jsx)(t.code,{children:"LLMAutoScaler.Spec.Metrics[0].Source.Name"}),", and default metric name is: ",(0,n.jsx)(t.code,{children:"vllm:num_requests_waiting"})]}),"\n",(0,n.jsx)(t.li,{children:"If there are multiple pods are selected, average metric value should be calculated."}),"\n",(0,n.jsxs)(t.li,{children:["If the specified metric can not resolved from the pod, for example, the pod is in the pending state. we should calculate the average value as following rules in order to prevent flapping.","\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"In scale up direction: use the 0 as the metric value for missing pods."}),"\n",(0,n.jsxs)(t.li,{children:["In scale down direction: use the ",(0,n.jsx)(t.code,{children:"LLMAutoScaler.Spec.Metrics[0].Threshold.High"})," as the metric value for missing pods."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(t.h3,{id:"basic-scaler",children:"Basic Scaler"}),"\n",(0,n.jsx)(t.p,{children:"The basic scaler is used to scale GPU workloads according to specified metric changes. The scaling rules are shown as following:"}),"\n",(0,n.jsxs)(t.table,{children:[(0,n.jsx)(t.thead,{children:(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.th,{children:"item"}),(0,n.jsx)(t.th,{children:"scale up"}),(0,n.jsx)(t.th,{children:"scale down"}),(0,n.jsx)(t.th,{children:"introducation"})]})}),(0,n.jsxs)(t.tbody,{children:[(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"scale step\xa0\xa0\xa0"}),(0,n.jsx)(t.td,{children:"1\xa0\xa0"}),(0,n.jsx)(t.td,{children:"1\xa0\xa0"}),(0,n.jsx)(t.td,{children:"\xa0only increase/reduce one replica in a scaling action, because the cost of gpu resource is really high\xa0"})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"cooldown seconds\xa0\xa0\xa0"}),(0,n.jsx)(t.td,{children:"600\xa0\xa0"}),(0,n.jsx)(t.td,{children:"1800\xa0\xa0"}),(0,n.jsx)(t.td,{children:"a waiting period after a scaling action for preventing frequent scaling"})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"stablizationwindow seconds\xa0\xa0\xa0"}),(0,n.jsx)(t.td,{children:"30\xa0\xa0"}),(0,n.jsx)(t.td,{children:"300\xa0\xa0"}),(0,n.jsx)(t.td,{children:"a lookback window that delays scaling decisions for avoiding premature scaling. and we hope scale-up to respond more quickly, while scale-down should occur more slowly. It means that scale-up is triggered only when the metric value exceeds the high threshold in 2 consecutive periods. and scale-down is triggered only when the metric value less than low threshold in 20 consecutive periods"})]})]})]}),"\n",(0,n.jsx)(t.p,{children:"All these values for basic scale strategy is pre-configured according to the experience. so the values maybe adjusted if more suitable values are found in the future use."}),"\n",(0,n.jsx)(t.p,{children:"The Scale Strategy Pseudocode"}),"\n",(0,n.jsx)(t.p,{children:"Inputs:"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"CurrentReplicas: Actual number of replicas for target workload, resolved from /scale subresource API."}),"\n",(0,n.jsx)(t.li,{children:"CurrentWaitingRequests: current waiting requests in inference queue, resolved from pods by metric scraper."}),"\n",(0,n.jsxs)(t.li,{children:["MinReplicas: The min number of replicas for target object, related field: ",(0,n.jsx)(t.code,{children:"LLMAutoScaler.Spec.MinReplicas"})]}),"\n",(0,n.jsxs)(t.li,{children:["MaxReplicas: The max number of replicas for target object, related field: ",(0,n.jsx)(t.code,{children:"LLMAutoScaler.Spec.MaxReplicas"})]}),"\n",(0,n.jsxs)(t.li,{children:["HighThreshold: expected high threshold of waiting requests, related field: ",(0,n.jsx)(t.code,{children:"LLMAutoScaler.Spec.Metrics[0].Threshold.High"})]}),"\n",(0,n.jsxs)(t.li,{children:["LowThreshold: expected low threshold of waiting requests, related field: ",(0,n.jsx)(t.code,{children:"LLMAutoScaler.Spec.Metrics[0].Threshold.Low"})]}),"\n",(0,n.jsx)(t.li,{children:"ScaleUpStep: the scale step of scaling up action, default value is 1"}),"\n",(0,n.jsx)(t.li,{children:"ScaleDownStep: the scale step of scaling down action, default value is 1"}),"\n",(0,n.jsx)(t.li,{children:"UpStabilizationWindowSeconds: the stabilization window seconds of scaling up action, default value is 30"}),"\n",(0,n.jsx)(t.li,{children:"DownStabilizationWindowSeconds: the stabilization window seconds of scaling down action, default value is 300"}),"\n",(0,n.jsx)(t.li,{children:"UpCoolDownSeconds: the cool down seconds of scaling up action, default value is 600"}),"\n",(0,n.jsx)(t.li,{children:"DownCoolDownSeconds: the cool down seconds of scaling down action, default value is 1800"}),"\n",(0,n.jsxs)(t.li,{children:["LastScaleTime: the timestamp for the last scaling action, related field: ",(0,n.jsx)(t.code,{children:"LLMAutoScaler.Status.LastScaleTime"})]}),"\n",(0,n.jsx)(t.li,{children:"MetricsHistory: include all latest metric values for each autoscaler, the type is: map[string][]timestampedMetricValue"}),"\n"]}),"\n",(0,n.jsx)(t.p,{children:"Outputs:"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"DesiredReplicas: Desired number of replicas for target workload. and the value will be used for scaling workload through /Scale subresource api."}),"\n"]}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-go",children:"type timestampedMetricValue struct {\n\tvalue int32\n\ttimestamp time.Time\n}\n\nfunc calculateDesiredReplicas(INPUTS) OUTPUTS{\n\t// 0. ensure metrics history updated for each reconcile\n\tdefer updateMetricsHistory(LLMAutoScaler.Name, CurrentWaitingRequests, MetricsHistory, UpStabilizationWindowSeconds, DownStabilizationWindowSeconds)\n\n\t// 1. calculate the elapsed time for cooldown check\n\tcooldownElapsed := time.Now().Sub(LastScaleTime)\n\n\t// 2. scale up logic\n\tif CurrentWaitingRequests > HighThreshold {\n\t\t// check stablization window\n\t\tif UpStabilizationWindowSeconds > 0 {\n\t\t\twindowMetrics := filterMetricsWithinWindow(LLMAutoScaler.Name, MetricsHistory, UpStabilizationWindowSeconds)\n\t\t\tminInWindow := min(windowMetrics)\n\t\t\tif minInWindow <= HighThreshold {\n\t\t\t\t// there is a metric value below high threshold, so skip scale up\n\t\t\t\treturn CurrentReplicas\n\t\t\t}\n\t\t}\n\n\t\t// check cooldown\n\t\tif cooldownElapsed > UpCoolDownSeconds {\n\t\t\treturn min(CurrentReplicas + ScaleUpStep, MaxReplicas)\n\t\t}\n\t}\n\n\t// 3. scale down logic\n\tif CurrentWaitingRequests < LowThreshold {\n\t\t// check stablization window\n\t\tif DownStabilizationWindowSeconds > 0 {\n\t\t\twindowMetrics := filterMetricsWithinWindow(LLMAutoScaler.Name, MetricsHistory, DownStabilizationWindowSeconds)\n\t\t\tmaxInWindow := max(windowMetrics)\n\t\t\tif maxInWindow >= LowThreshold {\n\t\t\t\t// there is a metric value above low threshold, so skip scale down\n\t\t\t\treturn CurrentReplicas\n\t\t\t}\n\t\t}\n\n\t\t// check cooldown\n\t\tif cooldownElapsed > DownCoolDownSeconds {\n\t\t\treturn max(CurrentReplicas - ScaleDownStep, MinReplicas)\n\t\t}\n\t}\n\n\t// 4. otherwise, skip scaling action\n\treturn CurrentReplicas\n}\n\nfunc filterMetricsWithinWindow(key, metricsHistory, stabilizationWindowSeconds) windowMetrics {\n\twindowStartTime := time.Now().Add(-time.Second * time.Duration(stabilizationWindowSeconds))\n\n\tfor i, timestampMetric := range metricsHistory[key] {\n\t\tif timestampMetric.timestamp.After(windowStartTime) {\n\t\t\twindowMetrics = append(windowMetrics, timestampMetric.Value)\n\t\t}\n\t}\n\n\treturn windowMetrics\n}\n\nfunc updateMetricsHistory(key, currentValue, metricsHistory, upStabilizationWindowSeconds, downStabilizationWindowSeconds) {\n\tupWindowStartTime := time.Now().Add(-time.Second * time.Duration(upStabilizationWindowSeconds))\n\tdownWindowStartTime := time.Now().Add(-time.Second * time.Duration(downStabilizationWindowSeconds))\n\n\tfoundStaleMetric := false\n\tstaleMetricIndex := 0\n\tfor i, timestampMetric := range metricsHistory[key] {\n\t\tif timestampMetric.timestamp.Before(upWindowStartTime) && timestampMetric.timestamp.Before(downWindowStartTime) {\n\t\t\tfoundStaleMetric = true\n\t\t\tstaleMetricIndex = i\n\t\t}\n\t}\n\n\t// use the stale metric slot for controlling the size of metric slice\n\tif foundStaleMetric {\n\t\tmetricsHistory[key][staleMetricIndex] = timestampedMetricValue{currentValue, time.Now()}\n\t} else {\n\t\tmetricsHistory[key] = append(metricsHistory[key], timestampedMetricValue{currentValue, time.Now())\n\t}\n}\n"})}),"\n",(0,n.jsx)(t.h2,{id:"alternatives",children:"Alternatives"}),"\n",(0,n.jsx)(t.h3,{id:"native-hpa",children:"Native HPA"}),"\n",(0,n.jsx)(t.p,{children:"Native HPA + Prometheus + Prometheus Adapter solution can also be used for scaling inference workloads of Kaito."}),"\n",(0,n.jsx)(t.h2,{id:"implementation-history",children:"Implementation History"}),"\n",(0,n.jsxs)(t.ul,{className:"contains-task-list",children:["\n",(0,n.jsxs)(t.li,{className:"task-list-item",children:[(0,n.jsx)(t.input,{type:"checkbox",disabled:!0})," ","06/10/2025: Open proposal PR"]}),"\n"]})]})}function u(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(d,{...e})}):d(e)}}}]);