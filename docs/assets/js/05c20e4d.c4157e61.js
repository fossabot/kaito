"use strict";(self.webpackChunkkaito_website=self.webpackChunkkaito_website||[]).push([[8],{7730:(e,t,i)=>{i.d(t,{A:()=>n});const n=i.p+"assets/images/workspace-scale-api-ec9f8d5bd28655c5c9c496fe6b7ae52d.png"},8411:(e,t,i)=>{i.d(t,{A:()=>n});const n=i.p+"assets/images/llm-auto-scaler-9629b5a2a655a402e6f937e918162b6c.png"},8453:(e,t,i)=>{i.d(t,{R:()=>o,x:()=>a});var n=i(6540);const s={},r=n.createContext(s);function o(e){const t=n.useContext(r);return n.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function a(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),n.createElement(r.Provider,{value:t},e.children)}},9303:(e,t,i)=>{i.r(t),i.d(t,{assets:()=>c,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>a,toc:()=>l});var n=i(4848),s=i(8453);const r={title:"Auto Scaler For Inference Workloads In Kaito",authors:["@rambohe-ch"],reviewers:["@Fei-Guo","@helayoty","@zhuangqh"],"creation-date":new Date("2025-06-10T00:00:00.000Z"),"last-updated":new Date("2025-06-30T00:00:00.000Z"),status:"provisional","see-also":null},o="Title",a={id:"proposals/auto-scaler-for-inference-workloads",title:"Auto Scaler For Inference Workloads In Kaito",description:"Auto Scaler for inference workloads in Kaito",source:"@site/docs/proposals/20250620-auto-scaler-for-inference-workloads.md",sourceDirName:"proposals",slug:"/proposals/auto-scaler-for-inference-workloads",permalink:"/kaito/docs/proposals/auto-scaler-for-inference-workloads",draft:!1,unlisted:!1,editUrl:"https://github.com/kaito-project/kaito/tree/main/website/docs/proposals/20250620-auto-scaler-for-inference-workloads.md",tags:[],version:"current",sidebarPosition:20250620,frontMatter:{title:"Auto Scaler For Inference Workloads In Kaito",authors:["@rambohe-ch"],reviewers:["@Fei-Guo","@helayoty","@zhuangqh"],"creation-date":"2025-06-10T00:00:00.000Z","last-updated":"2025-06-30T00:00:00.000Z",status:"provisional","see-also":null}},c={},l=[{value:"Summary",id:"summary",level:2},{value:"Motivation",id:"motivation",level:2},{value:"Goals",id:"goals",level:3},{value:"Non-Goals/Future Work",id:"non-goalsfuture-work",level:3},{value:"Proposal",id:"proposal",level:2},{value:"LLM-Auto-Scaler Architecture",id:"llm-auto-scaler-architecture",level:3},{value:"LLMAutoScaler CRD",id:"llmautoscaler-crd",level:3},{value:"Basic Scaler",id:"basic-scaler",level:3},{value:"Metrics Scraper",id:"metrics-scraper",level:4},{value:"Scaler Rule",id:"scaler-rule",level:4},{value:"Basic Scaler Pseudocode",id:"basic-scaler-pseudocode",level:4},{value:"Cron Scaler",id:"cron-scaler",level:3},{value:"Cron Scaler Pseudocode",id:"cron-scaler-pseudocode",level:4},{value:"Alternatives",id:"alternatives",level:2},{value:"Native HPA",id:"native-hpa",level:3},{value:"Acknowledgments",id:"acknowledgments",level:2},{value:"Implementation History",id:"implementation-history",level:2}];function d(e){const t={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",img:"img",input:"input",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(t.h1,{id:"title",children:"Title"}),"\n",(0,n.jsx)(t.p,{children:"Auto Scaler for inference workloads in Kaito"}),"\n",(0,n.jsx)(t.h2,{id:"summary",children:"Summary"}),"\n",(0,n.jsx)(t.p,{children:"As the number of waiting inference requests increases, it is necessary to scale more inference instances in order to prevent blocking inference requests. On the other hand, if the number of waiting inference requests declines, we should consider reducing inference instances to improve GPU resource utilization.\nNative Kubernetes has provided HPA capability to scale workload instances automatically as the metrics change, but HPA depends on third-party components (like Prometheus, Prometheus Adapter, etc.) to collect custom metrics from the source pods."}),"\n",(0,n.jsx)(t.p,{children:"In this proposal, we hope to support a customized auto-scaler which is specialized for scaling GPU workloads for Kaito. The auto-scaler is designed for a minimalistic configuration experience, with most parameters pre-tuned for optimal performance. This allows users to easily get started without requiring specialized knowledge of LLM."}),"\n",(0,n.jsx)(t.p,{children:"By the way, the scale subresource API for workspace CRD is addressed in this proposal:"}),"\n",(0,n.jsx)(t.h2,{id:"motivation",children:"Motivation"}),"\n",(0,n.jsxs)(t.p,{children:["LLM inference service is a basic and widely-used feature in Kaito, and Kaito community interest in auto-scaler for inference workloads continues to intensify. Related issues: ",(0,n.jsx)(t.a,{href:"https://github.com/kaito-project/kaito/issues/306",children:"#306"}),", ",(0,n.jsx)(t.a,{href:"https://github.com/kaito-project/kaito/issues/1104",children:"#1104"}),"."]}),"\n",(0,n.jsx)(t.p,{children:"From the technical perspective, it's a good idea to provide auto-scaler capability because the auto-scaler of inference workloads dynamically adjusts the number of inference instances based on request volume\u2014scaling up during traffic spikes to improve inference speed, and scaling down during low demand to minimize GPU resource waste. Furthermore, for workloads with predictable, recurring traffic patterns, a time-based scaler can proactively adjust capacity, ensuring resources are ready before they are needed."}),"\n",(0,n.jsx)(t.p,{children:"The auto-scaler solution for Kaito should be shown as follows:"}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{alt:"auto-scaler",src:i(7730).A+"",width:"3542",height:"2117"})}),"\n",(0,n.jsx)(t.p,{children:"We will divide this auto-scaler feature into two parts as follows:"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsxs)(t.li,{children:["Part one: support scale subresource API for workspace, so different auto-scaler solutions such as KEDA, HPA, etc. can be integrated with Kaito to manage inference workloads dynamically. This part is addressed in another proposal: ",(0,n.jsx)(t.a,{href:"https://github.com/kaito-project/kaito/pull/1184",children:"https://github.com/kaito-project/kaito/pull/1184"}),"."]}),"\n",(0,n.jsx)(t.li,{children:"Part two: support a customized auto-scaler for Kaito. The auto-scaler is designed for a minimalistic configuration experience, with most parameters pre-tuned for optimal performance. This allows users to easily get started without requiring specialized knowledge of LLM. This part will be addressed in this proposal. We name this auto-scaler as llm-auto-scaler, which will support both reactive (metric-based) and proactive (time-based) scaling."}),"\n"]}),"\n",(0,n.jsx)(t.p,{children:"To ensure ease of use, the specialized auto-scaler is hosted in an independent repo (kaito-project/llm-auto-scaler). At the same time, the llm-auto-scaler component can work with Kaito without depending on any third-party components."}),"\n",(0,n.jsx)(t.h3,{id:"goals",children:"Goals"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"llm-auto-scaler is a specialized auto-scaler for scaling gpu workloads automatically, and can integrate with kaito to work."}),"\n",(0,n.jsx)(t.li,{children:"It is flexible enough to support multiple scaling strategies. This proposal introduces two primary scalers: a metric-based scaler for reactive scaling and a cron-based scaler for proactive, scheduled scaling."}),"\n"]}),"\n",(0,n.jsx)(t.h3,{id:"non-goalsfuture-work",children:"Non-Goals/Future Work"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"Only support to configure one metric for basic scale strategy, multiple metrics will be supported in future version."}),"\n",(0,n.jsx)(t.li,{children:"The time efficiency of the auto-scaler is not within the scope of this proposal, as it is influenced by mutliple external factors, including GPU node provisioning, LLM image pulling, etc."}),"\n",(0,n.jsx)(t.li,{children:"Only support scale vllm workload, and non-vllm is not covered."}),"\n"]}),"\n",(0,n.jsx)(t.h2,{id:"proposal",children:"Proposal"}),"\n",(0,n.jsx)(t.h3,{id:"llm-auto-scaler-architecture",children:"LLM-Auto-Scaler Architecture"}),"\n",(0,n.jsxs)(t.p,{children:["The llm-auto-scaler component determines the desired number of replicas based on the configured scaling strategy in the ",(0,n.jsx)(t.code,{children:"LLMAutoScaler"})," CRD. For metric-based scaling, it scrapes metrics from the inference pods. For time-based scaling, it evaluates cron schedules. The scaler controller then calculates the desired replica count and scales the workspace replicas through the ",(0,n.jsx)(t.code,{children:"/scale"})," subresource API. The detailed auto-scaler architecture is shown in the following figure:"]}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{alt:"auto-scaler",src:i(8411).A+"",width:"4010",height:"2012"})}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.strong,{children:"LLMAutoScaler CRD"}),": Defines the auto-scaler configuration, including the scaling strategy (metric-based or time-based), target resource, and scaling parameters."]}),"\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.strong,{children:"Metrics Scraper"}),": A module of the llm-auto-scaler used for scraping metrics from inference pods. This component is active only when a ",(0,n.jsx)(t.code,{children:"BasicScaler"})," (metric-based) strategy is configured."]}),"\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.strong,{children:"Scale Strategy"}),": The proposal supports two primary scaling strategies: ",(0,n.jsx)(t.code,{children:"BasicScaler"})," for reactive scaling based on metrics, and ",(0,n.jsx)(t.code,{children:"CronScaler"})," for proactive scaling based on a predefined schedule. These strategies determine the algorithm used to calculate the desired number of replicas."]}),"\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.strong,{children:"Scaler Controller"}),": The core logic that integrates the selected scale strategy (either with scraped metrics or cron schedules) to calculate the desired replicas and invokes the ",(0,n.jsx)(t.code,{children:"/scale"})," subresource API of the target workspace."]}),"\n"]}),"\n",(0,n.jsx)(t.h3,{id:"llmautoscaler-crd",children:"LLMAutoScaler CRD"}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{children:'type ScalerRule struct {\n\t// CooldownSeconds defines the waiting period after a scaling action.\n\t// +optional\n    CooldownSeconds *int32\n\n    // StabilizationWindowSeconds defines the lookback window to prevent premature scaling.\n    // +optional\n    StabilizationWindowSeconds *int32\n\n\t// ScaleStep defines the number of replicas increase/reduce during a scaling action.\n\tScaleStep *int32\n}\n\ntype ProtocolType string\n\nconst (\n\tHTTP  ProtocolType = "http"\n\tHTTPS ProtocolType = "https"\n)\n\n// MetricSource defines the way to fetch the specific metric\ntype MetricSource struct {\n\t// Name identifies the specific metric to monitor.\n\t// If unset, vllm:num_requests_waiting will be used.\n\tName string\n\n\t// Protocol specify the protocol for accessing pods, http and https are supported.\n\t// if unset, http will be used.\n\tProtocol ProtocolType\n\n\t// Port specify the port of pods /metrics endpoint.\n\t// if unset, 5000 will be used.\n\tPort string\n\n\t// Path specify the path of the metric endpoint.\n\t// if unset, /metrics will be used.\n\tPath string\n}\n\ntype MetricThreshold struct {\n\t// High means the upper threshold, when the value of the monitored metric exceeds this number,\n\t// the autoscaler will decide to scale up.\n\tHigh int32\n\n\t// Low mens the lower threshold. when the value of the monitored metric drops below this number,\n\t// the autoscaler will scale down.\n\tLow int32\n}\n\ntype Metric struct {\n\t// metric identifies the way to fetch target metric\n\t// if unset, scaler will fetch metric(vllm:num_requests_waiting) from http://{pod-ip}:5000/metrics endpoint. \n\t// and pod-ip is retrieved from pods that related the ScaleTargetRef.\n\t// +optional\n\tSource MetricSource\n\n\t// threshold defines the boundaries used to trigger scaling actions basd on the monitored metric.\n\tThreshold MetricThreshold\n}\n\ntype BasicScaler struct {\n\t// metrics contains the specifications for how to fetching the specified metric and scale threshold of metric.\n\t// only one metric is supported currently, and multiple metrics will be supported in future version.\n\tMetrics []Metric\n}\n\ntype CronPolicy struct {\n\t// Name is used to specify the name of the policy.\n\tName string\n\t// The schedule in Cron format, see https://en.wikipedia.org/wiki/Cron.\n\tSchedule string\n\t// TargetReplicas is used to specify the desired number of replicas.\n\tTargetReplicas int32\n}\n\ntype CronScaler struct {\n\t// Policies is a list of cron policies that define the cron scaling schedule.\n\tPolicies []CronPolicy\n}\n\ntype Scaler struct {\n\t// Basic represents a basic auto scaler that scales up/down workloads according to metric changes.\n\t// At the same time, basic auto scaler works in a passive mode, it means inference requests will be blocked before new inference workloads added.\n\t// +optional\n\tBasic *BasicScaler\n\n\t// Cron represents a cron auto scaler that scales up/down workloads at specified time.\n\t// And cron auto scaler works in a proactive mode, the inference workloads will be ready before the inference requests spikes.\n\tCron *CronScaler\n}\n\ntype LLMAutoScalerSpec struct {\n\t// scaleTargetRef points to the target resource to scale. e.g. Workspace\n\tScaleTargetRef autoscalingv2api.CrossVersionObjectReference\n\n\t// MinReplicas is the lower limit for the number of replicas to which the autoscaler\n\t// can scale down. Default value is 1.\n\t// +optional\n\tMinReplicas *int32\n\n\t// MaxReplicas is the upper limit for the number of replicas to which the autoscaler can scale up.\n\t// It cannot be less that MinReplicas.\n\tMaxReplicas int32\n\n\t// Scaler defines the scaling strategy, which can be either metric-based (Basic) or time-based (Cron).\n\tScaler Scaler\n\n\t// ScaleUpRule defines the rules for scaling up. and cron scaler will skip this field.\n\t// if unset, ScaleStep will set to 1, CooldownSeconds to 600, and StabilizationWindowSeconds to 30.\n\t// +optional\n\tScaleUpRule ScalerRule\n\n\t// ScaleDownRule defines the rules for scaling down. and cron scaler will skip this field.\n\t// if unset, ScaleStep will set to 1, CooldownSeconds to 1800, and StabilizationWindowSeconds to 300.\n\t// +optional\n\tScaleDownRule ScalerRule\n}\n\ntype LLMAutoScalerStatus struct {\n\t// lastScaleTime is the last time the LLMAutoScaler scaled the number of inference workloads,\n\t// used by the autoscaler to control how often the number of inference workloads is changed.\n\t// +optional\n\tLastScaleTime *metav1.Time\n\n\t// currentReplicas is current number of replicas of inference workloads managed by this autoscaler,\n\t// as last seen by the autoscaler.\n\t// +optional\n\tCurrentReplicas int32\n\n\t// desiredReplicas is the desired number of replicas of inference workloads managed by this autoscaler,\n\t// as last calculated by the autoscaler.\n\tDesiredReplicas int32\n\n\t// Conditions is the set of conditions required for this autoscaler to scale its target,\n\t// and indicates whether or not those conditions are met.\n\tConditions []metav1.Condition\n}\n\ntype LLMAutoScaler struct {\n\tmetav1.TypeMeta\n\tmetav1.ObjectMeta\n\n\tSpec   LLMAutoScalerSpec\n\tStatus LLMAutoScalerStatus\n}\n'})}),"\n",(0,n.jsx)(t.h3,{id:"basic-scaler",children:"Basic Scaler"}),"\n",(0,n.jsx)(t.h4,{id:"metrics-scraper",children:"Metrics Scraper"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsxs)(t.li,{children:["Metrics Scraper fetches specified metrics from pods' /metrics endpoint according to ",(0,n.jsx)(t.code,{children:"LLMAutoScaler.Spec.Scaler.Basic.Metrics"})," at 15s interval. only one metric is supported in the first version."]}),"\n",(0,n.jsxs)(t.li,{children:["metrics endpoint url: ",(0,n.jsx)(t.code,{children:"LLMAutoScaler.Spec.Scaler.Basic.Metrics[0].Source.Protocol://{pod ip}:LLMAutoScaler.Spec.Scaler.Basic.Metrics[0].Source.Port/metrics"}),", default value is: ",(0,n.jsx)(t.code,{children:"http://{pod ip}:5000/metrics"})]}),"\n",(0,n.jsxs)(t.li,{children:["pod ip: get ip from pods that referenced by ",(0,n.jsx)(t.code,{children:"InferenceAutoScaler.Spec.ScaleTargetRef"})]}),"\n",(0,n.jsxs)(t.li,{children:["resolve metric value from response by ",(0,n.jsx)(t.code,{children:"LLMAutoScaler.Spec.Scaler.Basic.Metrics[0].Source.Name"}),", and default metric name is: ",(0,n.jsx)(t.code,{children:"vllm:num_requests_waiting"})]}),"\n",(0,n.jsx)(t.li,{children:"If there are multiple pods are selected, average metric value should be calculated."}),"\n",(0,n.jsxs)(t.li,{children:["If the specified metric can not resolved from the pod, for example, the pod is in the pending state. we should calculate the average value as following rules in order to prevent flapping.","\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"In scale up direction: use the 0 as the metric value for missing pods."}),"\n",(0,n.jsxs)(t.li,{children:["In scale down direction: use the ",(0,n.jsx)(t.code,{children:"LLMAutoScaler.Spec.Scaler.Basic.Metrics[0].Threshold.High"})," as the metric value for missing pods."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(t.h4,{id:"scaler-rule",children:"Scaler Rule"}),"\n",(0,n.jsx)(t.p,{children:"The basic scaler is used to scale GPU workloads according to specified metric changes. it means that basic scaler is a passive scaling.\nand the scaling rules will be integrated to calculate desired replicas."}),"\n",(0,n.jsxs)(t.table,{children:[(0,n.jsx)(t.thead,{children:(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.th,{children:"item"}),(0,n.jsx)(t.th,{children:"scale up(default value)"}),(0,n.jsx)(t.th,{children:"scale down(default value)"}),(0,n.jsx)(t.th,{children:"introduication"})]})}),(0,n.jsxs)(t.tbody,{children:[(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"scale step\xa0\xa0\xa0"}),(0,n.jsx)(t.td,{children:"1\xa0\xa0"}),(0,n.jsx)(t.td,{children:"1\xa0\xa0"}),(0,n.jsx)(t.td,{children:"\xa0only increase/reduce one replica in a scaling action, because the cost of gpu resource is really high\xa0"})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"cooldown seconds\xa0\xa0\xa0"}),(0,n.jsx)(t.td,{children:"600\xa0\xa0"}),(0,n.jsx)(t.td,{children:"1800\xa0\xa0"}),(0,n.jsx)(t.td,{children:"a waiting period after a scaling action for preventing frequent scaling"})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"stablizationwindow seconds\xa0\xa0\xa0"}),(0,n.jsx)(t.td,{children:"30\xa0\xa0"}),(0,n.jsx)(t.td,{children:"300\xa0\xa0"}),(0,n.jsx)(t.td,{children:"a lookback window that delays scaling decisions for avoiding premature scaling. and we hope scale-up to respond more quickly, while scale-down should occur more slowly. It means that scale-up is triggered only when the metric value exceeds the high threshold in 2 consecutive periods. and scale-down is triggered only when the metric value less than low threshold in 20 consecutive periods"})]})]})]}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"Because cron scaler is used to scale workloads at scheduled time, so scaler rule will be skipped by cron scaler."}),"\n"]}),"\n",(0,n.jsx)(t.h4,{id:"basic-scaler-pseudocode",children:"Basic Scaler Pseudocode"}),"\n",(0,n.jsx)(t.p,{children:"Inputs:"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"CurrentReplicas: Actual number of replicas for target workload, resolved from /scale subresource API."}),"\n",(0,n.jsx)(t.li,{children:"CurrentWaitingRequests: current waiting requests in inference queue, resolved from pods by metric scraper."}),"\n",(0,n.jsxs)(t.li,{children:["MinReplicas: The min number of replicas for target object, related field: ",(0,n.jsx)(t.code,{children:"LLMAutoScaler.Spec.MinReplicas"})]}),"\n",(0,n.jsxs)(t.li,{children:["MaxReplicas: The max number of replicas for target object, related field: ",(0,n.jsx)(t.code,{children:"LLMAutoScaler.Spec.MaxReplicas"})]}),"\n",(0,n.jsxs)(t.li,{children:["HighThreshold: expected high threshold of waiting requests, related field: ",(0,n.jsx)(t.code,{children:"LLMAutoScaler.Spec.Scaler.Basic.Metrics[0].Threshold.High"})]}),"\n",(0,n.jsxs)(t.li,{children:["LowThreshold: expected low threshold of waiting requests, related field: ",(0,n.jsx)(t.code,{children:"LLMAutoScaler.Spec.Scaler.Basic.Metrics[0].Threshold.Low"})]}),"\n",(0,n.jsxs)(t.li,{children:["ScaleUpStep: the scale step of scaling up action, related field: ",(0,n.jsx)(t.code,{children:"LLMAutoScaler.Spec.ScaleUpRule.ScaleStep"})]}),"\n",(0,n.jsxs)(t.li,{children:["ScaleDownStep: the scale step of scaling down action, related field: ",(0,n.jsx)(t.code,{children:"LLMAutoScaler.Spec.ScaleDownRule.ScaleStep"})]}),"\n",(0,n.jsxs)(t.li,{children:["UpStabilizationWindowSeconds: the stabilization window seconds of scaling up action, related field: ",(0,n.jsx)(t.code,{children:"LLMAutoScaler.Spec.ScaleUpRule.StabilizationWindowSeconds"})]}),"\n",(0,n.jsxs)(t.li,{children:["DownStabilizationWindowSeconds: the stabilization window seconds of scaling down action, related field: ",(0,n.jsx)(t.code,{children:"LLMAutoScaler.Spec.ScaleDownRule.StabilizationWindowSeconds"})]}),"\n",(0,n.jsxs)(t.li,{children:["UpCoolDownSeconds: the cool down seconds of scaling up action, related field: ",(0,n.jsx)(t.code,{children:"LLMAutoScaler.Spec.ScaleUpRule.CoolDownSeconds"})]}),"\n",(0,n.jsxs)(t.li,{children:["DownCoolDownSeconds: the cool down seconds of scaling down action, related field: ",(0,n.jsx)(t.code,{children:"LLMAutoScaler.Spec.ScaleDownRule.CoolDownSeconds"})]}),"\n",(0,n.jsxs)(t.li,{children:["LastScaleTime: the timestamp for the last scaling action, related field: ",(0,n.jsx)(t.code,{children:"LLMAutoScaler.Status.LastScaleTime"})]}),"\n",(0,n.jsx)(t.li,{children:"MetricsHistory: include all latest metric values for each autoscaler, the type is: map[string][]timestampedMetricValue"}),"\n"]}),"\n",(0,n.jsx)(t.p,{children:"Outputs:"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"DesiredReplicas: Desired number of replicas for target workload. and the value will be used for scaling workload through /Scale subresource api."}),"\n"]}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-go",children:"type timestampedMetricValue struct {\n\tvalue int32\n\ttimestamp time.Time\n}\n\nfunc calculateDesiredReplicas(INPUTS) OUTPUTS{\n\t// 0. ensure metrics history updated for each reconcile\n\tdefer updateMetricsHistory(LLMAutoScaler.Name, CurrentWaitingRequests, MetricsHistory, UpStabilizationWindowSeconds, DownStabilizationWindowSeconds)\n\n\t// 1. calculate the elapsed time for cooldown check\n\tcooldownElapsed := time.Now().Sub(LastScaleTime)\n\n\t// 2. scale up logic\n\tif CurrentWaitingRequests > HighThreshold {\n\t\t// check stablization window\n\t\tif UpStabilizationWindowSeconds > 0 {\n\t\t\twindowMetrics := filterMetricsWithinWindow(LLMAutoScaler.Name, MetricsHistory, UpStabilizationWindowSeconds)\n\t\t\tminInWindow := min(windowMetrics)\n\t\t\tif minInWindow <= HighThreshold {\n\t\t\t\t// there is a metric value below high threshold, so skip scale up\n\t\t\t\treturn CurrentReplicas\n\t\t\t}\n\t\t}\n\n\t\t// check cooldown\n\t\tif cooldownElapsed > UpCoolDownSeconds {\n\t\t\treturn min(CurrentReplicas + ScaleUpStep, MaxReplicas)\n\t\t}\n\t}\n\n\t// 3. scale down logic\n\tif CurrentWaitingRequests < LowThreshold {\n\t\t// check stablization window\n\t\tif DownStabilizationWindowSeconds > 0 {\n\t\t\twindowMetrics := filterMetricsWithinWindow(LLMAutoScaler.Name, MetricsHistory, DownStabilizationWindowSeconds)\n\t\t\tmaxInWindow := max(windowMetrics)\n\t\t\tif maxInWindow >= LowThreshold {\n\t\t\t\t// there is a metric value above low threshold, so skip scale down\n\t\t\t\treturn CurrentReplicas\n\t\t\t}\n\t\t}\n\n\t\t// check cooldown\n\t\tif cooldownElapsed > DownCoolDownSeconds {\n\t\t\treturn max(CurrentReplicas - ScaleDownStep, MinReplicas)\n\t\t}\n\t}\n\n\t// 4. otherwise, skip scaling action\n\treturn CurrentReplicas\n}\n\nfunc filterMetricsWithinWindow(key, metricsHistory, stabilizationWindowSeconds) windowMetrics {\n\twindowStartTime := time.Now().Add(-time.Second * time.Duration(stabilizationWindowSeconds))\n\n\tfor i, timestampMetric := range metricsHistory[key] {\n\t\tif timestampMetric.timestamp.After(windowStartTime) {\n\t\t\twindowMetrics = append(windowMetrics, timestampMetric.Value)\n\t\t}\n\t}\n\n\treturn windowMetrics\n}\n\nfunc updateMetricsHistory(key, currentValue, metricsHistory, upStabilizationWindowSeconds, downStabilizationWindowSeconds) {\n\tupWindowStartTime := time.Now().Add(-time.Second * time.Duration(upStabilizationWindowSeconds))\n\tdownWindowStartTime := time.Now().Add(-time.Second * time.Duration(downStabilizationWindowSeconds))\n\n\tfoundStaleMetric := false\n\tstaleMetricIndex := 0\n\tfor i, timestampMetric := range metricsHistory[key] {\n\t\tif timestampMetric.timestamp.Before(upWindowStartTime) && timestampMetric.timestamp.Before(downWindowStartTime) {\n\t\t\tfoundStaleMetric = true\n\t\t\tstaleMetricIndex = i\n\t\t}\n\t}\n\n\t// use the stale metric slot for controlling the size of metric slice\n\tif foundStaleMetric {\n\t\tmetricsHistory[key][staleMetricIndex] = timestampedMetricValue{currentValue, time.Now()}\n\t} else {\n\t\tmetricsHistory[key] = append(metricsHistory[key], timestampedMetricValue{currentValue, time.Now())\n\t}\n}\n"})}),"\n",(0,n.jsx)(t.h3,{id:"cron-scaler",children:"Cron Scaler"}),"\n",(0,n.jsx)(t.p,{children:"The Cron Scaler provides a proactive, time-based scaling mechanism that adjusts the number of replicas at scheduled times. Unlike the Basic Scaler, which reacts to metric changes, the Cron Scaler allows you to align resource allocation with predictable traffic patterns, such as scaling up before business hours and scaling down during nights or weekends. This approach ensures that capacity is available before demand spikes, improving responsiveness and user experience."}),"\n",(0,n.jsxs)(t.p,{children:["Each cron policy defines a ",(0,n.jsx)(t.code,{children:"schedule"})," in standard cron format and a ",(0,n.jsx)(t.code,{children:"targetReplicas"})," count. The autoscaler will continuously monitor the clock and, when the current time matches a policy's schedule, it will adjust the workload's replica count to the specified target."]}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"Take the following policies as an example:"}),"\n"]}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-yaml",children:'policies:\n  # The "scale-up" policy increases the number of replicas to 5 at 8:00 AM every day\n  # to handle peak traffic during business hours.\n  - name: "scale-up"\n    schedule: "0 8 * * *" # Cron schedule for 8:00 AM daily\n    targetReplicas: 5\n  # The "scale-down" policy reduces the number of replicas to 1 at 1 minute past midnight (00:01)\n  # every day to conserve resources during off-peak hours.\n  - name: "scale-down"\n    schedule: "1 0 * * *" # Cron schedule for 00:01 AM daily\n    targetReplicas: 1\n'})}),"\n",(0,n.jsxs)(t.p,{children:["The ",(0,n.jsx)(t.code,{children:"scale-up"})," policy will scale the number of replicas to 5 at 08:00 every day, and the ",(0,n.jsx)(t.code,{children:"scale-down"})," policy will scale the number of replicas to 1 at 00:01 every day."]}),"\n",(0,n.jsx)(t.h4,{id:"cron-scaler-pseudocode",children:"Cron Scaler Pseudocode"}),"\n",(0,n.jsx)(t.p,{children:"Inputs:"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"CurrentReplicas: Actual number of replicas for the target workload."}),"\n",(0,n.jsxs)(t.li,{children:["CronPolicies: The list of cron policies defined in ",(0,n.jsx)(t.code,{children:"LLMAutoScaler.Spec.Scaler.Cron.Policies"}),"."]}),"\n",(0,n.jsx)(t.li,{children:"MinReplicas: The minimum number of replicas for the target object."}),"\n",(0,n.jsx)(t.li,{children:"MaxReplicas: The maximum number of replicas for the target object."}),"\n"]}),"\n",(0,n.jsx)(t.p,{children:"Outputs:"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"DesiredReplicas: Desired number of replicas for the target workload."}),"\n"]}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-go",children:'import (\n\t"time"\n\t"github.com/robfig/cron/v3"\n)\n\nfunc calculateDesiredReplicasCron(INPUTS) OUTPUTS {\n\tnow := time.Now()\n\tvar matchingReplicas []int32\n\n\t// Find all policies that match the current time\n\tfor _, policy := range CronPolicies {\n\t\tschedule, err := cron.ParseStandard(policy.Schedule)\n\t\tif err != nil {\n\t\t\t// Log error and skip policy\n\t\t\tcontinue\n\t\t}\n\n\t\tnextRun := schedule.Next(now.Add(-time.Minute)) // Check if the last run was within the last minute\n\t\tif now.Truncate(time.Minute).Equal(nextRun.Truncate(time.Minute)) {\n\t\t\tmatchingReplicas = append(matchingReplicas, policy.TargetReplicas)\n\t\t}\n\t}\n\n\t// If no policy matches, no change in replicas\n\tif len(matchingReplicas) == 0 {\n\t\treturn CurrentReplicas\n\t}\n\n\t// If multiple policies match, pick the one with the highest replica count\n\t// to ensure capacity for the highest-demand schedule.\n\tdesired := matchingReplicas[0]\n\tfor i := 1; i < len(matchingReplicas); i++ {\n\t\tif matchingReplicas[i] > desired {\n\t\t\tdesired = matchingReplicas[i]\n\t\t}\n\t}\n\n\t// Clamp the desired replicas within the min/max boundaries\n\tif desired < MinReplicas {\n\t\tdesired = MinReplicas\n\t}\n\tif desired > MaxReplicas {\n\t\tdesired = MaxReplicas\n\t}\n\n\treturn desired\n}\n'})}),"\n",(0,n.jsx)(t.h2,{id:"alternatives",children:"Alternatives"}),"\n",(0,n.jsx)(t.h3,{id:"native-hpa",children:"Native HPA"}),"\n",(0,n.jsx)(t.p,{children:"Native HPA + Prometheus + Prometheus Adapter solution can also be used for scaling inference workloads of Kaito."}),"\n",(0,n.jsx)(t.h2,{id:"acknowledgments",children:"Acknowledgments"}),"\n",(0,n.jsxs)(t.p,{children:["We would like to acknowledge the Kubernetes ",(0,n.jsx)(t.a,{href:"https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/",children:"Horizontal Pod Autoscaler (HPA)"})," and ",(0,n.jsx)(t.a,{href:"https://github.com/vllm-project/aibrix",children:"AIBrix"})," for providing inspiration and reference points in designing the LLMAutoScaler API. Their approaches to scaling workloads dynamically based on metrics have been instrumental in shaping this proposal."]}),"\n",(0,n.jsx)(t.h2,{id:"implementation-history",children:"Implementation History"}),"\n",(0,n.jsxs)(t.ul,{className:"contains-task-list",children:["\n",(0,n.jsxs)(t.li,{className:"task-list-item",children:[(0,n.jsx)(t.input,{type:"checkbox",disabled:!0})," ","06/10/2025: Open proposal PR"]}),"\n"]})]})}function h(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(d,{...e})}):d(e)}}}]);